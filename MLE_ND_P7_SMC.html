<!DOCTYPE HTML>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=0.9*device-width">
    <title>MLE_ND_P7_SMC</title>
    <script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script>
    <script>$(function () {
    sagecell.makeSagecell({inputLocation:'div.linked',linked:true,evalButtonText:'Run Linked Cells'});  
    sagecell.makeSagecell({inputLocation:'div.sage',evalButtonText:'Run'});
    });
    </script>
  </head>
  <style>
  @import url('https://fonts.googleapis.com/css?family=Orbitron|Roboto');
  body {background-color:#f6e4e4;}; a, p {color:#a44a4a; font-family:'Roboto';} 
  h1 {color:#cd5c5c; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;} 
  h2, h3 {color:slategray; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;}
  h4 {color:#cd5c5c; font-family:'Roboto';}
  .sagecell .CodeMirror-scroll {min-height:3em; max-height:70em;}
  .sagecell table.table_form tr.row-a {background-color:lightgray;} 
  .sagecell table.table_form tr.row-b {background-color:#f6e4e4;}
  .sagecell table.table_form td {padding:5px 15px; color:#a44a4a; font-family:'Roboto';}
  .sagecell_sessionOutput, .sagecell_sessionOutput pre {color:#a44a4a; font-family:'Roboto';}
  </style>  
  <body>
    <h1>Machine Learning Engineer Nanodegree</h1>
    <h2>Deep Learning</h2>
    <h1>&#x1F4D1; &nbsp;P7: Building a Student Intervention System</h1>
    <h2>Introduction</h2>      
    <h3>Resources</h3>
<a href="https://scikit-learn.org/stable/index.html">&#x1F578;scikit-learn. Machine Learning in Python&nbsp;</a>
<a href="http://scipy-lectures.org/">&#x1F578;Scipy Lecture Notes&nbsp;</a><br/>
    <h3>Code Library</h3> 
<div class="linked"><script type="text/x-sage">
import numpy,pandas,time,pylab; pylab.style.use('seaborn-whitegrid')
import warnings; from sklearn.exceptions import DataConversionWarning
for el in [FutureWarning,UserWarning,RuntimeWarning,DataConversionWarning]: warnings.filterwarnings("ignore",category=el)
from sklearn import linear_model,svm
from sklearn.model_selection import train_test_split,GridSearchCV,ShuffleSplit
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier
from sklearn.neighbors import KNeighborsClassifier,RadiusNeighborsClassifier
from sklearn.ensemble import BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier,RandomForestClassifier
from sklearn.metrics import f1_score,make_scorer
</script></div>
    <p></p>
    <h3>Question 1 - Classification vs. Regression</h3>
<i>The goal for this project is to identify students who might need early intervention before they fail to graduate.<br/>
Which type of supervised learning problem is this, classification or regression? Why?</i>
    <h3>Answer 1</h3>
For simplicity, the border between regression and classification can be described in this way.<br/>
- <b>Classification</b>: predict the values of discrete or categorical targets.<br/>
- <b>Regression</b>: predict the values of continuous targets.<br/>
This supervised learning problem is in the classification field. We should predict the labels for the students (<i>yes</i> or <i>no</i>) for the feature <i>passed</i>.
    <h2>Exploring the Data</h2>
We will start from loading the student data.<br/>
Note that the last column from this dataset <i>passed</i> will be our target label (whether the student graduated or didn't graduate).<br/>
All other columns are features about each student.<br/>
<div class="linked"><script type="text/x-sage">
path='https://raw.githubusercontent.com/OlgaBelitskaya/machine_learning_engineer_nd009/master/Machine_Learning_Engineer_ND_P7/'
data=pandas.read_csv(path+'student-data.csv')
print ('Data were read successfully!'); data.describe().T
</script></div>
    <p></p> 
Let's begin by investigating the dataset to determine how many students we have information on, and learn about the graduation rate among these students. <br/>
We will need to compute the following:<br/>
- The total number of students, <i>n_students</i>.<br/>
- The total number of features for each student, <i>n_features</i>.<br/>
- The number of those students who passed, <i>n_passed</i>.<br/>
- The number of those students who failed, <i>n_failed</i>.<br/>
- The graduation rate of the class, <i>grad_rate</i>, in percent (%).
<div class="linked"><script type="text/x-sage">
n_students,n_features=len(data),len(list(data.T.index))
n_passed=len(data[data['passed']=='yes']); n_failed=len(data[data['passed']=='no'])
grad_rate=n_passed*100.0/n_students
print ("Total number of students: {}".format(n_students))
print ("Number of features: {}".format(n_features))
print ("Number of students who passed: {}".format(n_passed))
print ("Number of students who failed: {}".format(n_failed))
print ("Graduation rate of the class: {:.2f}%".format(float(grad_rate)))
</script></div>
    <p></p>
    <h2>Preparing the Data</h2>
In this section, we will prepare the data for modeling, training and testing.
    <h3>Identify feature and target columns</h3>
It is often the case that the data you obtain contains non-numeric features. 
This can be a problem because most machine learning algorithms expect numeric data to perform computations with.
<div class="linked"><script type="text/x-sage">
feature_cols=list(data.columns[:-1]); target_col=data.columns[-1] 
print ("Feature columns:")
for i in range(4): print(feature_cols[i*8:(i+1)*8])
print ("\nTarget column: {}".format(target_col))
X_all,y_all=data[feature_cols],data[target_col]
print ("Feature values:"); X_all.head().T
</script></div>
    <p></p>
    <h3>Preprocess Feature Columns</h3>
As we can see, there are several non-numeric columns that need to be converted! <br/>
Many of them are simply <b>yes/no</b>, e.g. <i>internet</i>. These can be reasonably converted into <b>1/0</b> (binary) values.<br/>
Other columns, like <i>Mjob</i> and <i>Fjob</i>, have more than two values, and are known as categorical variables.<br/>
The recommended way to handle such a column is to create as many columns as possible values (e.g. <i>Fjob_teacher, Fjob_other, Fjob_services</i>, etc.),<br/> and assign a 1 to one of them and 0 to all others.<br/>
These generated columns are sometimes called dummy variables, and we will use the <i>pandas.get_dummies()</i> function to perform this transformation.
<div class="linked"><script type="text/x-sage">
def preprocess_features(X):
    output=pandas.DataFrame(index=X.index)
    for col,col_data in X.iteritems():
        if col_data.dtype==object: 
            col_data=col_data.replace(['yes','no'],[1,0])
        if col_data.dtype==object:
            col_data=pandas.get_dummies(col_data,prefix=col)  
        output=output.join(col_data)    
    return output
X_all=preprocess_features(X_all)
print ("Processed feature columns (%d total features):"%len(X_all.columns))
for i in range(6): print(list(X_all.columns)[i*8:(i+1)*8])
</script></div>
    <p></p>
    <h3>Training and Testing Data Split</h3>
So far, we have converted all categorical features into numeric values.<br/>
Next, we will randomly shuffle and split the data <i>(X_all, y_all)</i> into training and testing subsets by the following steps:<br/>
- Use 300 training points (approximately 75%) and 95 testing points (approximately 25%).<br/>
- Set the <i>random_state</i> parameter for the function(s) we use, if provided.<br/>
- Store the results in <i>X_train, X_test, y_train, y_test</i>.
<div class="linked"><script type="text/x-sage">
num_train=300; num_test=X_all.shape[0]-num_train
X_train,X_test,y_train,y_test=train_test_split(X_all,y_all,test_size=1.0*num_test/len(X_all),random_state=1)
print ("Training set has {} samples.".format(X_train.shape[0]))
print ("Testing set has {} samples.".format(X_test.shape[0]))
</script></div>
    <p></p>
    <h2>Training and Evaluating Models</h2>
In this section, you will choose 3 <b>supervised learning</b> models that are appropriate for this problem and available in <i>scikit-learn</i>.<br/>
You will first discuss the reasoning behind choosing these three models by considering what you know about the data and each model's strengths and weaknesses. <br/>
You will then fit the model to varying sizes of training data (100 data points, 200 data points, and 300 data points) and measure the F1 score. <br/>
You will need to produce three tables (one for each model) that shows: <br/>
- training set size, training time, prediction time, F1 score on the training set, and F1 score on the testing set.<br/>
The following <b>supervised learning</b> models are currently available in <i>scikit-learn</i> that you may choose from:<br/>
<i style="font-family:'Roboto';">- Gaussian Naive Bayes (GaussianNB)<br/>
- Decision Trees<br/>
- Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting)<br/>
- K-Nearest Neighbors (KNeighbors)<br/>
- Stochastic Gradient Descent (SGDC)<br/>
- Support Vector Machines (SVM)<br/>
- Logistic Regression</i>
    <h3>Question 2 - Model Application</h3>
<i>List three supervised learning models that are appropriate for this problem. For each model chosen:<br/>
- Describe one real-world application in industry where the model can be applied. (You may need to do a small bit of research for this â€” give references!)<br/>
- What are the strengths of the model; when does it perform well?<br/>
- What are the weaknesses of the model; when does it perform poorly?<br/>
- What makes this model a good candidate for the problem, given what you know about the data?</i>
    <h3>Answer 2</h3>
Let's make quick checking F1 scores of the mentioned models:
<div class="linked"><script type="text/x-sage">
clf=[linear_model.LogisticRegression(solver='liblinear',multi_class='ovr'),
     linear_model.LogisticRegressionCV(solver='liblinear',multi_class='ovr'),
     linear_model.SGDClassifier(max_iter=1000,tol=0.00001),
     svm.LinearSVC(),svm.SVC(gamma='scale',C=10.0,kernel='poly'),svm.NuSVC(gamma='scale',kernel='poly'),
     KNeighborsClassifier(),RadiusNeighborsClassifier(radius=8),DecisionTreeClassifier(),ExtraTreeClassifier(),
     BaggingClassifier(),RandomForestClassifier(n_estimators=64),AdaBoostClassifier(),GradientBoostingClassifier()]
print('F1-scores')
for c in clf:
    c.fit(X_train,y_train); y_test_pred=c.predict(X_test)
    print(c.__class__.__name__+' - '+str(f1_score(y_test,y_test_pred,pos_label='yes')))
</script></div>
    <p></p>
I have chosen the following models: <i>LogisticRegressionCV, svm.SVC, RadiusNeighborsClassifier</i>.<br/>
In the experiments, they usually have a higher result than others.<br/>
Let's have a look at their applications and characteristics:<br/>
1) <i>LogisticRegressionCV</i>.<br/>
<b>Applications</b>: <br/>
<b>Strengths</b>: <br/>
<b>Weaknesses</b>: <br/>
2) <i>svm.SVC</i>.<br/>
<b>Applications</b>: <br/>
<b>Strengths</b>: <br/>
<b>Weaknesses</b>: <br/>
3) <i>RadiusNeighborsClassifier</i>.<br/>
<b>Applications</b>: <br/>
<b>Strengths</b>: <br/>
<b>Weaknesses</b>: <br/>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
    <h3>Additional Code Cell</h3>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p> 
  </body>
</html>