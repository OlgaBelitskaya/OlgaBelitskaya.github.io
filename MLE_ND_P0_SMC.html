<!DOCTYPE HTML>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=0.9*device-width">
    <title>MLE_ND_P0_SMC</title>
    <script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script>
    <script>$(function () {
    sagecell.makeSagecell({inputLocation:'div.linked',linked:true,evalButtonText:'Run Linked Cells'});  
    sagecell.makeSagecell({inputLocation:'div.sage',evalButtonText:'Run'});
    });
    </script>
  </head>
  <style>
  @import url('https://fonts.googleapis.com/css?family=Orbitron|Roboto');
  body {background-color:oldlace;}; a {color:darksalmon; font-family:'Roboto';} 
  h1 {color:#ff603b; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;} 
  h2,h3 {color:slategray; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;}
  h4 {color:#ff603b; font-family:'Roboto';}
  .sagecell .CodeMirror-scroll {min-height:3em; max-height:48em;}
  </style>  
  <body>
    <h1>Machine Learning Engineer Nanodegree</h1>
    <h2>Introduction and Foundations</h2>
    <h1>&#x1F4D1; &nbsp;P0: Titanic Survival Exploration</h1>
In 1912, the ship RMS Titanic struck an iceberg on its maiden voyage and sank, resulting in the deaths of most of its passengers and crew.<br/>
In this introductory project, we will explore a subset of the RMS Titanic passenger manifest to determine which features best predict whether someone survived or did not survive.
    <h2>Getting Started</h2>
    <h3>Resources</h3>
<a href="https://www.udacity.com/course/intro-to-data-science--ud359">&#x1F578; Intro to Data Science. Udacity &nbsp;</a>
<a href="http://scipy-lectures.org/packages/statistics/index.html">&#x1F578; Statistics in Python. Scipy Lecture Notes &nbsp;</a>
<a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">&#x1F578; A Visual Introduction to Machine Learning. Part 1 &nbsp;</a>
<a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics">&#x1F578; The scikit-learn metrics</a>
    <h3>Code Tools</h3>
<div class="linked"><script type="text/x-sage">
import numpy,pandas,pylab; pylab.style.use('ggplot')
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
# https://github.com/udacity/machine-learning/blob/master/projects/titanic_survival_exploration/visuals.py
def filter_data(data,condition):
    field,sign,value=condition.split(" ")
    try: value=float(value)
    except: value=value.strip("\'\"")
    if sign==">": exp=data[field]>value
    elif sign=="<": exp=data[field]<value
    elif sign==">=": exp=data[field]>=value
    elif sign=="<=": exp=data[field]<=value
    elif sign=="==": exp=data[field]==value
    elif sign=="!=": exp=data[field]!=value
    else: raise Exception("Invalid comparison operator. Only >, <, >=, <=, ==, != allowed.")
    return data[exp].reset_index(drop=True)    
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
def survival_stats(data,outcomes,key,filters=[]):
    if (key not in data.columns.values):
        print ("'{}' is not a feature of the Titanic data. Did you spell something wrong?".format(key)); return False
    if (key=='Cabin' or key=='PassengerId' or key=='Ticket'):
        print ("'{}' has too many unique categories to display! Try a different feature.".format(key)); return False
    all_data=data.T.append(outcomes).T
    for condition in filters: all_data=filter_data(all_data,condition)
    all_data=all_data[[key,'Survived']]; pylab.figure(figsize=(12,5))
    if (key=='Age' or key=='Fare'):
        all_data=all_data[~pandas.isnull(all_data[key])]
        min_value=all_data[key].min(); max_value=all_data[key].max(); value_range=max_value-min_value
        if(key=='Fare'): bins=numpy.arange(0,all_data['Fare'].max()+20,20)
        if(key=='Age'): bins=numpy.arange(0,all_data['Age'].max()+10,10)
        nonsurv_vals=all_data[all_data['Survived']==0][key].reset_index(drop=True).astype('float')
        surv_vals=all_data[all_data['Survived']==1][key].reset_index(drop=True).astype('float')
        pylab.hist(nonsurv_vals,bins=bins,alpha=0.8,color='#FF7F50',label='Did not survive')
        pylab.hist(surv_vals,bins=bins,alpha=0.8,color='#338DD4',label='Survived')
        pylab.xlim(0,bins.max()); pylab.legend(framealpha=0.8)
    else:
        if (key=='Pclass'): values=numpy.arange(1,4)
        if (key=='Parch' or key=='SibSp'): values=numpy.arange(0,numpy.max(data[key])+1)
        if (key=='Embarked'): values=['C','Q','S']
        if (key=='Sex'): values=['male','female']
        frame=pandas.DataFrame(index=numpy.arange(len(values)),columns=(key,'Survived','NSurvived'))
        for i,value in enumerate(values):
            frame.loc[i]=[value,len(all_data[(all_data['Survived']==1) & (all_data[key]==value)]),
                          len(all_data[(all_data['Survived']==0) & (all_data[key]==value)])]
        bar_width=0.4
        for i in numpy.arange(len(frame)):
            nonsurv_bar =pylab.bar(i-bar_width,frame.loc[i]['NSurvived'],width=bar_width,color='#FF7F50')
            surv_bar=pylab.bar(i,frame.loc[i]['Survived'],width=bar_width,color='#338DD4')
            pylab.xticks(numpy.arange(len(frame)),values)
            pylab.legend((nonsurv_bar[0],surv_bar[0]),('Did not survive','Survived'),framealpha=0.8)
    pylab.xlabel(key); pylab.ylabel('Number of Passengers')
    pylab.title('Passenger Survival Statistics With the \'%s\' Feature'%(key)); pylab.show()
    if sum(pandas.isnull(all_data[key])):
        nan_outcomes=all_data[pandas.isnull(all_data[key])]['Survived']
        print ("Passengers with missing '{}' values: {} ({} survived, {} did not survive)"\
               .format(key,len(nan_outcomes),sum(nan_outcomes==1),sum(nan_outcomes==0)))
</script></div>
    <p></p>
To begin working with the RMS Titanic passenger data, we'll first need to import the functionality we need, and load our data into a pandas DataFrame.
<br/>
Run the code cell below to load our data and display the first few entries (passengers) for examination using the .head() function.
<div class="linked"><script type="text/x-sage">
path='https://raw.githubusercontent.com/OlgaBelitskaya/machine_learning_engineer_nd009/master/Machine_Learning_Engineer_ND_P0/'
full_data=pandas.read_csv(path+'titanic_data.csv'); full_data.head()
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
full_data.info();
</script></div>
    <p></p>
From a sample of the RMS Titanic data, we can see the various features present for each passenger on the ship:<br/>
<b>Survived:</b> Outcome of survival (0 = No; 1 = Yes)<br/>
<b>Pclass:</b> Socio-economic class (1 = Upper class; 2 = Middle class; 3 = Lower class)<br/>
<b>Name:</b> Name of passenger<br/>
<b>Sex:</b> Sex of the passenger<br/>
<b>Age:</b> Age of the passenger (Some entries contain NaN)<br/>
<b>SibSp:</b> Number of siblings and spouses of the passenger aboard<br/>
<b>Parch:</b> Number of parents and children of the passenger aboard<br/>
<b>Ticket:</b> Ticket number of the passenger<br/>
<b>Fare:</b> Fare paid by the passenger<br/>
<b>Cabin:</b> Cabin number of the passenger (Some entries contain NaN)<br/>
<b>Embarked:</b> Port of embarkation of the passenger (C = Cherbourg; Q = Queenstown; S = Southampton)<br/>
Since we're interested in the outcome of survival for each passenger or crew member,<br/>
we can remove the <b>Survived</b> feature from this dataset and store it as its own separate variable outcomes.<br/> 
We will use these outcomes as our prediction targets.      
<div class="linked"><script type="text/x-sage">
outcomes=full_data['Survived']; data=full_data.drop('Survived',axis=1); data.head()
</script></div>
    <p></p>
The very same sample of the RMS Titanic data now shows the <b>Survived</b> feature removed from the DataFrame.<br/>
Note that the passenger data and the outcomes of survival are now paired. That means for any passenger <i>data.loc[i]</i>, they have the survival <i>outcomes[i]</i>.<br/>
To measure the performance of our predictions, we need a metric to score our predictions against the true outcomes of survival - the function <i>accuracy_score()</i>.<br/>
Since we are interested in how accurate our predictions are, we will calculate the proportion of passengers where our prediction of their survival is correct.<br/>
<b>Think:</b> Out of the first five passengers, if we predict that all of them survived, what would you expect the accuracy of our predictions to be?      
<div class="linked"><script type="text/x-sage">
def accuracy_score(truth, pred):
    if len(truth)==len(pred): 
        return "Predictions have an accuracy of {:.2f}%.".format((truth==pred).mean()*100)    
    else:
        return "Number of predictions does not match number of outcomes!"
predictions=pandas.Series(numpy.ones(5,dtype=int)); accuracy_score(outcomes[:5,],predictions)
</script></div>
    <p></p>
      <h2>Making Predictions</h2>
If we were asked to make a prediction about any passenger aboard the RMS Titanic whom we knew nothing about,<br/> 
then the best prediction we could make would be that they did not survive.<br/>
This is because we can assume that a majority of the passengers (more than 50%) did not survive the ship sinking.<br/>
The <i>predictions_0()</i> function below will always predict that a passenger did not survive.
<div class="linked"><script type="text/x-sage">
def predictions_0(data):
    predictions=[]
    for _,passenger in data.iterrows(): predictions.append(0)
    return pandas.Series(predictions)
predictions=predictions_0(data)
</script></div>
    <p></p>
      <h3>Question 1</h3>
Using the RMS Titanic data, how accurate would a prediction be that none of the passengers survived?
      <h3>Answer 1</h3>     
<div class="linked"><script type="text/x-sage">
accuracy_score(outcomes,predictions)
</script></div>
    <p></p>
Let's take a look at whether the feature <b>Sex</b> has any indication of survival rates among passengers using the <i>survival_stats()</i> function.<br/>
This function is defined in the <i>visualizations.py</i>. Python script included with this project.<br/> 
The first two parameters passed to the function are the RMS Titanic data and passenger survival outcomes, respectively.<br/> 
The third parameter indicates which feature we want to plot survival statistics across.  
<div class="linked"><script type="text/x-sage">
survival_stats(data,outcomes,'Sex')
</script></div>
    <p></p>
Examining the survival statistics, a large majority of males did not survive the ship sinking.<br/>
However, a majority of females did survive the ship sinking.<br/> 
Let's build on our previous prediction: if a passenger was female, then we will predict that they survived.<br/>
Otherwise, we will predict the passenger did not survive.
<div class="linked"><script type="text/x-sage">
def predictions_1(data):
    predictions=[]
    for _,passenger in data.iterrows():
        if passenger['Sex']=='female': predictions.append(1)
        else: predictions.append(0)
    return pandas.Series(predictions)
predictions=predictions_1(data)
</script></div>
    <p></p>
    <h3>Question 2</h3>
How accurate would a prediction be that all female passengers survived and the remaining passengers did not survive (<i>prediction_1()</i>)?
    <h3>Answer 2</h3>     
<div class="linked"><script type="text/x-sage">
accuracy_score(outcomes,predictions)
</script></div>
    <p></p> 
Using just the <b>Sex</b> feature for each passenger, we are able to increase the accuracy of our predictions by a significant margin.<br/>
Now, let's consider using an additional feature to see if we can further improve our predictions.<br/>
For example, consider all of the male passengers aboard the RMS Titanic: can we find a subset of those passengers that had a higher rate of survival?<br/>
Let's start by looking at the <b>Age</b> of each male, by again using the <i>survival_stats()</i> function.<br/> 
This time, we'll use the fourth parameter to filter out the data so that only passengers with the <b>Sex</b> 'male' will be included.     
<div class="linked"><script type="text/x-sage">
survival_stats(data,outcomes,'Age',["Sex == 'male'"])
</script></div>
    <p></p>
Examining the survival statistics, the majority of males younger than 10 survived the ship sinking, whereas most males age 10 or older did not survive the ship sinking.<br/>
Let's continue to build on our previous prediction: if a passenger was female, then we will predict they survive.<br/> 
If a passenger was male and younger than 10, then we will also predict they survive. Otherwise, we will predict they do not survive.      
<div class="linked"><script type="text/x-sage">
def predictions_2(data):
    predictions=[]
    for _,passenger in data.iterrows():
        if passenger['Sex']=='female': predictions.append(1)
        elif passenger['Sex']=='male' and passenger['Age']<10: predictions.append(1)
        else: predictions.append(0)
    return pandas.Series(predictions)
predictions=predictions_2(data)
</script></div>
    <p></p>
    <h3>Question 3</h3>
How accurate would a prediction be that all female passengers and all male passengers younger than 10 survived (<i>prediction_2()</i>)?
    <h3>Answer 3</h3>     
<div class="linked"><script type="text/x-sage">
accuracy_score(outcomes,predictions)
</script></div>
    <p></p>
Adding the feature <b>Age</b> as a condition in conjunction with <b>Sex</b> improves the accuracy by a small margin more than with simply using the feature <b>Sex</b> alone.<br/>
Now we can try to find a series of features and conditions to split the data on to obtain an outcome prediction accuracy of at least 80%.<br/>
This may require multiple features and multiple levels of conditional statements to succeed.<br/>
We can use the same feature multiple times with different conditions.<br/>
There are some experiments and the function <i>prediction_final()</i> as a result:
<div class="linked"><script type="text/x-sage">
survival_stats(data,outcomes,'SibSp',["Sex == 'male'","Age < 15"])
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
survival_stats(data,outcomes,'Pclass',["Sex == 'male'","Age < 15"])
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
def predictions_final(data):   
    predictions=[]
    for _,passenger in data.iterrows():
        if (passenger['Sex']=='female'): predictions.append(1)                
        elif passenger['Pclass'] in [1,2] and (passenger['Age']<16 or passenger['Age']>75): predictions.append(1)            
        elif passenger['Age']<15 and passenger['SibSp']<3: predictions.append(1)            
        else: predictions.append(0)
    return pandas.Series(predictions) 
predictions=predictions_final(data)
</script></div>
    <p></p>
    <h3>Question 4</h3>
Describe the steps you took to implement the final prediction model so that it got an accuracy of at least 80%.<br/>
What features did you look at? Were certain features more informative than others?<br/>
Which conditions did you use to split the survival outcomes into the data? How accurate are your predictions?
    <h3>Answer 4</h3>
The final set of features <b>Sex</b>, <b>Age</b>, <b>SibSp</b> and <b>Pclass</b> are the most informative on my opinion.<br/>
As we noted the percentage of survivors of passengers is much higher among women than among men, and it was used in our predictions.<br/>
Next, I proceed from the assumption that because of humanitarian reasons people rescue children and elders at first.<br/> Unfortunately, this was only valid for the passengers of the first and second classes in this dataset.<br/>
And the latest clarification, which overcomes the border of 80% in prediction accuracy:<br/>
if a family has more than three children, absolutely all the family may not be survived in catastrophic situations and in an atmosphere of panic.     
<div class="linked"><script type="text/x-sage">
accuracy_score(outcomes,predictions)
</script></div>
    <p></p>
    <h3>Scikit-learn Metrics</h3>
There is a set of metrics in the <b>scikit-learn</b> package for evaluation the quality of predictions.
<div class="linked"><script type="text/x-sage">
from sklearn.metrics import recall_score,accuracy_score,precision_score
print ("Predictions have an accuracy of {:.2f}%.".format(accuracy_score(outcomes, predictions)*100))
print ("Predictions have a recall score equal to {:.2f}%.".format(recall_score(outcomes, predictions)*100))
print ("Predictions have a precision score equal to {:.2f}%.".format(precision_score(outcomes,predictions)*100))
</script></div>
    <p></p>
The evaluation terminology:

$accuracy \ = \ \frac{number \ of \ people \ that \ are \ correctly \ predicted \ as \ survived \ or \ non−survived}{number \ of \ all \ people \ in \ the \ dataset}$     
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
  </body>
</html>