<!DOCTYPE HTML>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=0.9*device-width">
    <title>MLE_ND_P8_SMC</title>
    <script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script>
    <script>$(function () {
    sagecell.makeSagecell({inputLocation:'div.linked',linked:true,evalButtonText:'Run Linked Cells'});  
    sagecell.makeSagecell({inputLocation:'div.sage',evalButtonText:'Run'});
    });
    </script>
  </head>
  <style>
  @import url('https://fonts.googleapis.com/css?family=Orbitron|Roboto');
  body {background-color:aliceblue;}; a, p {color:darkblue; font-family:'Roboto';} 
  h1 {color:#4169E1; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;} 
  h2, h3 {color:slategray; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;}
  h4 {color:#4169E1; font-family:'Roboto';}
  .sagecell .CodeMirror-scroll {min-height:3em; max-height:70em;}
  .sagecell table.table_form tr.row-a {background-color:lightgray;} 
  .sagecell table.table_form tr.row-b {background-color:aliceblue;}
  .sagecell table.table_form td {padding:5px 15px; color:darkblue; font-family:'Roboto';}
  .sagecell_sessionOutput, .sagecell_sessionOutput pre {color:darkblue; font-family:'Roboto';}
  </style>  
  <body>
    <h1>Machine Learning Engineer Nanodegree</h1>
    <h2>Additional Project</h2>
    <h1>&#x1F4D1; &nbsp;P8: Analyzing the NYC Subway Dataset</h1>
    <h2>Links and Code Library</h2>     
    <h3>Resources</h3>
<a href="https://scikit-learn.org/stable/index.html">&#x1F578;scikit-learn. Machine Learning in Python&nbsp;</a>
<a href="http://scipy-lectures.org/">&#x1F578;Scipy Lecture Notes&nbsp;</a><br/>
<a href=" https://ocw.mit.edu/resources/res-6-009-how-to-process-analyze-and-visualize-data-january-iap-2012/lectures-and-labs/MITRES_6_009IAP12_lab3a.pdf">&#x1F578;Hypothesis Testing - MIT OpenCourseWare&nbsp;</a>
<a href="https://statistics.laerd.com/premium-sample/mwut/mann-whitney-test-in-spss-2.php">&#x1F578;Assumptions of the Mann-Whitney U test&nbsp;</a><br/>     
    <h3>Code Library</h3> 
<div class="linked"><script type="text/x-sage">
import numpy,pandas,pylab,seaborn,sympy,time; pylab.style.use('seaborn-whitegrid')
import warnings; from sklearn.exceptions import DataConversionWarning
for el in [FutureWarning,UserWarning,RuntimeWarning,DataConversionWarning]: warnings.filterwarnings("ignore",category=el)
import statsmodels.api as sm
from scipy.stats import mannwhitneyu,linregress
from sklearn import linear_model
from sklearn.metrics import r2_score
from sklearn.preprocessing import normalize
</script></div>
    <p></p>
    <h3>Set of Functions</h3>
<div class="linked"><script type="text/x-sage">
def normalize_features(data):
    mu,sigma=data.mean(),data.std()    
    if (sigma==0).any():
        raise Exception("One of the features had the same value for all samples and could not be normalized." +  
                        "Do not include features with only a single value in the model.")        
    return (data-mu)/sigma,mu,sigma
def compute_cost(features,values,theta):    
    sum_of_square_errors=numpy.square(numpy.dot(features,theta)-values).sum()
    return sum_of_square_errors/(2*len(values))
def gradient_descent(features,values,theta,alpha,num_iterations):
    cost_history=[]
    for i in range(num_iterations):
        delta=(numpy.dot((values-numpy.dot(features,theta)),features))/len(values)
        theta=theta+alpha*delta
        cost_history.append(compute_cost(features,values,theta))
    return theta,pandas.Series(cost_history)
def predictions(dataframe,numeric,target,alpha=0.1,num_iterations=50):
    features=dataframe[numeric]; values=dataframe[target]
    dummy_units=pandas.get_dummies(dataframe['UNIT'],prefix='unit')
    features=features.join(dummy_units)
    features,mu,sigma=normalize_features(features); features['ones']=numpy.ones(len(values))
    features_array=numpy.array(features); values_array=numpy.array(values)
    theta_gradient_descent=numpy.zeros(len(features.columns))
    theta_gradient_descent,cost_history=\
    gradient_descent(features_array,values_array,theta_gradient_descent,alpha,num_iterations)    
    predictions=numpy.dot(features_array,theta_gradient_descent)
    return predictions,theta_gradient_descent
</script></div>
    <p></p>
    <h2>Statistical Test</h2> 
    <h3>Data Extraction and Description</h3>
<div class="linked"><script type="text/x-sage">
path='https://raw.githubusercontent.com/OlgaBelitskaya/machine_learning_engineer_nd009/master/Machine_Learning_Engineer_ND_P8/'
data=pandas.read_csv(path+'turnstile_data_master_with_weather.csv').drop('index',int(1))
data.describe().T
</script></div>
    <p></p>   
<div class="linked"><script type="text/x-sage">
pretty_print('Data Medians')
pandas.DataFrame(data.median(),columns=['median'])
</script></div>
    <p></p>
    <h3>Test Selection</h3>
    <h4>Question 1.1</h4>
<i>Which statistical test did you use to analyze the NYC subway data? Did you use a one-tail or a two-tail P value?<br/>
What is the null hypothesis? What is your p-critical value?</i>
    <h4>Answer 1.1</h4>
The Mann-Whitney U Test to compare the ridership of NYC subway in rainy and non-rainy days is a good choice.<br/>
The column <i>ENTRIESn_hourly</i> will be a target and the column <i>rain</i> - a feature.<br/>
I will test the <b>null hypothesis</b>: distributions of ridership in NYC subway are the same for rainy and non-rainy days.<br/>Another variant of this hypothesis could be: the difference between ridership medians / means for rainy and non-rainy days is equal to zero.<br/>
I will use a two-tailed test to find the statistical significance in both possible directions of interest.<br/>
Let's setup the p-critical value is equal to 0.05: we will reject the null hypothesis at the confidence level of 95%.  
    <h4>Question 1.2</h4> 
<i>Why is this statistical test applicable to the dataset?<br/>
In particular, consider the assumptions that the test is making about the distribution of ridership in the two samples.</i>
    <h4>Answer 1.2</h4>
The Mann-Whitney U Test is a non-parametric alternative test for comparing two sample medians / means (or two distributions) that come from the same population.<br/>
I have noted that the distribution of <i>ENTRIESn_hourly</i> is not normal so I cannot use the t-test in this case.<br/>
The features' specifics for the Mann-Whitney U test:<br/>
1) one dependent variable is measured at the continuous or ordinal level (<i>ENTRIESn_hourly</i>),<br/>
2) one independent variable consists of two categorical, independent groups (<i>rain</i>),<br/>
3) independence of observations (true for this data, the samples do not affect each other),<br/>
4) if two distributions have the same shapes, the test determines differences in the medians / means of two groups, if they have different shapes - <br/>
differences in the distributions of two groups (distributions in our case, the shapes are similar but with different levels),<br/>
5) two samples under consideration could not have the same number of observations (true for this data).
    <h3>Test Execution</h3>   
At first, I will describe two samples, then I will run the test and display the results.
<div class="linked"><script type="text/x-sage">
pretty_print("ENTRIESn_hourly in rainy days")
rainy_entries_hourly=data['ENTRIESn_hourly'][data['rain']==1]
pandas.DataFrame(rainy_entries_hourly.describe())
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
pretty_print("ENTRIESn_hourly in non-rainy days")
non_rainy_entries_hourly=data['ENTRIESn_hourly'][data['rain']==0]
pandas.DataFrame(non_rainy_entries_hourly.describe())
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
with_rain_median=numpy.median(rainy_entries_hourly)
without_rain_median=numpy.median(non_rainy_entries_hourly)
with_rain_mean=numpy.mean(rainy_entries_hourly)
without_rain_mean=numpy.mean(non_rainy_entries_hourly)
U,p=mannwhitneyu(rainy_entries_hourly,non_rainy_entries_hourly,alternative='two-sided')
print ("Mean for rainy days: {:.0f}".format(with_rain_mean))
print ("Mean for non-rainy days: {:.0f}".format(without_rain_mean))
print ("Median for rainy days: {:.0f}".format(with_rain_median))
print ("Median for non-rainy days: {:.0f}".format(without_rain_median))
print ("Number of rainy days: {:.0f}".format(len(rainy_entries_hourly)))
print ("Number for non-rainy days: {:.0f}".format(len(non_rainy_entries_hourly)))
print ("Mann-Whitney U test: U = {:.0f}, p = {:.5f}".format(U,p))
</script></div>
    <p></p>
    <h4>Question 1.3</h4>
<i>What results did you get from this statistical test?<br/> 
These should include the following numerical values: p-values, as well as the medians / means for each of the two samples under test.</i>
    <h4>Answer 1.3</h4>
<div class="linked"><script type="text/x-sage">
table([['Mean for rainy days','Mean for non-rainy days','Median for rainy days','Median for non-rainy days',
        'Number of rainy days','Number for non-rainy days','p-value of Mann-Whitney U-Test'],
       [with_rain_mean,without_rain_mean,with_rain_median,without_rain_median,
        len(rainy_entries_hourly),len(non_rainy_entries_hourly),p]])
</script></div>
    <p></p>
    <h4>Question 1.4</h4>
<i>What is the significance and interpretation of these results?</i>
    <h4>Answer 1.4</h4>
The median / mean values of ENTRIESn_hourly in rainy days is only a little bit larger than in non-rainy days.<br/>
I cannot determine whether the null hypothesis is rejected or not based on the difference between each pair of values.<br/>
The Mann-Whitney U-Test detects more informative results on whether the null hypothesis is true or not.<br/>
The p-value of this test is less than the p-critical value.<br/>
Therefore, I can confirm that the null hypothesis is rejected with 95% of confidence level.
    <h2>Linear Regression</h2> 
In this section, I will use the improved dataset <i>turnstile_weather_v2.csv</i>. Let's load and describe it.      
<div class="linked"><script type="text/x-sage">
data2=pandas.read_csv(path+'turnstile_weather_v2.csv')
data2.head().T
</script></div>
    <p></p>
    <h4>Question 2.1</h4> 
<i>What approach did you use to compute the coefficients theta and produce prediction for ENTRIESn_hourly in your regression model:<br/>
- ols using statsmodels or sklearn<br/>
- gradient descent using sklearn<br/>
- or something different?</i>
    <h4>Answer 2.1</h4>
To produce predictions I would like to try<br/> 
- a simple ordinary least squares model (ols statsmodels), <br/>
- stochastic gradient descent regression (SGDRegressor sklearn) and <br/>
- the set of built functions (<i>normalize_features(), compute_cost(), gradient_descent(), predictions()</i>). <br/>
It will be interesting to compare the results.
    <h4>Predictions by the Set of Built Functions</h4>
<div class="linked"><script type="text/x-sage">
numeric2=['rain','hour','weekday','meantempi']
predictions2=predictions(data2,numeric2,'ENTRIESn_hourly')
pretty_print ("Predictions for 'ENTRIESn_hourly'")
pandas.DataFrame(predictions2[0],columns=['predictions']).head()
</script></div>
    <p></p>
The code cell below produce predictions and display the histogram of residuals (the differences between the observed values and the estimated values).
<div class="linked"><script type="text/x-sage">
pylab.figure(figsize=(12,5)); pylab.title("Histogram of Residauls 'ENTRIESn_hourly', Theta Gradient Descent",fontsize=15)
(data2['ENTRIESn_hourly']-predictions2[0]).hist(bins=200,color='#4169E1',edgecolor='black',alpha=0.5); pylab.show()
</script></div>
    <p></p>
    <h4>Predictions by the OLS Model</h4>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
    <h3>Additional Code Cell</h3>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p> 
  </body>
</html>