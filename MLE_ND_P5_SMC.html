<!DOCTYPE HTML>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=0.9*device-width">
    <title>MLE_ND_P5_SMC</title>
    <script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script>
    <script>$(function () {
    sagecell.makeSagecell({inputLocation:'div.linked',linked:true,evalButtonText:'Run Linked Cells'});  
    sagecell.makeSagecell({inputLocation:'div.sage',evalButtonText:'Run'});
    });
    </script>
  </head>
  <style>
  @import url('https://fonts.googleapis.com/css?family=Orbitron|Roboto');
  body {background-color:#b8e2fc;}; a, p {color:royalblue; font-family:'Roboto';} 
  h1 {color:#191970; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;} 
  h2, h3 {color:slategray; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;}
  h4 {color:#191970; font-family:'Roboto';}
  .sagecell .CodeMirror-scroll {min-height:3em; max-height:70em;}
  .sagecell table.table_form tr.row-a {background-color:lightgray;} 
  .sagecell table.table_form tr.row-b {background-color:#b8e2fc;}
  .sagecell table.table_form td {padding:5px 15px; color:royalblue; font-family:'Roboto';}
  .sagecell_sessionOutput, .sagecell_sessionOutput pre {color:royalblue; font-family:'Roboto';}
  </style>  
  <body>
    <h1>Machine Learning Engineer Nanodegree</h1>
    <h2>Deep Learning</h2>
    <h1>&#x1F4D1; &nbsp;P5: Build a Digit Recognition Program</h1>
    <h2>Step 0: Load and Create Datasets</h2>      
    <h3>Resources</h3>
<a href="https://scikit-learn.org/stable/index.html">&#x1F578;scikit-learn. Machine Learning in Python&nbsp;</a>
<a href="http://scipy-lectures.org/">&#x1F578;Scipy Lecture Notes&nbsp;</a><br/>
    <h3>Code Library</h3> 
<div class="linked"><script type="text/x-sage">
import numpy,pandas,skimage,pylab,urllib,random,h5py,sys; pylab.style.use('seaborn-whitegrid')
import warnings; from sklearn.exceptions import DataConversionWarning
for el in [FutureWarning,UserWarning,DataConversionWarning]: warnings.filterwarnings("ignore",category=el)
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn import datasets,metrics,preprocessing
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
def ohe(x,categories='auto'): return preprocessing.OneHotEncoder(categories=categories).fit(x.reshape(-1,1))\
                                     .transform(x.reshape(-1,1)).toarray().astype('int16')
def sas(X,y): return train_test_split(X,y,test_size=0.2,random_state=1)
def random_rotate(img): return skimage.transform.rotate(img,numpy.random.randint(-20,20))
def label5d(label):
    zeros=numpy.full((5-len(label)),10)
    if len(label) >= 5: return label
    else: return numpy.array(numpy.concatenate((zeros,label),axis=0))
def concatenate(images,labels,image_size):
    i=0   
    concat_images=numpy.array(numpy.zeros((image_size,image_size*5))).reshape(1,image_size,image_size*5)
    concat_labels=numpy.array(numpy.zeros(5))   
    while i<images.shape[0]:
        if images.shape[0]-i<=5: 
            image=images[i]; label=labels[i]            
            for  j  in range(images.shape[0]-i-1): 
                image=numpy.concatenate((image,images[i+j+1]),axis=1)
                label=numpy.concatenate((label,labels[i+j+1]),axis=0)                
            label=label5d(label); image=skimage.transform.resize(image,(image_size,image_size*5))            
            concat_images=numpy.vstack((concat_images,[image]))
            concat_labels=numpy.vstack((concat_labels,label))           
            i+=5            
        else:
            random_n=numpy.random.randint(3,6); image=images[i]; label=labels[i]           
            for k in range(random_n-1):
                image=numpy.concatenate((image,images[i+k+1]),axis=1)
                label=numpy.concatenate((label,labels[i+k+1]),axis=0)                
            label=label5d(label); image=skimage.transform.resize(image,(image_size,image_size*5))           
            concat_images=numpy.vstack((concat_images,[image]))
            concat_labels=numpy.vstack((concat_labels,label)).astype('int16')           
            i+=random_n            
    concat_images=concat_images[1:]; concat_labels=concat_labels[1:]
    return concat_images,concat_labels
</script></div>
    <p></p> 
    <h3>Experimental Datasets</h3>
<h4>Dataset #1. Scikit-learn. Digits.</h4>
<div class="linked"><script type="text/x-sage">
digits=datasets.load_digits(n_class=10)
X_train1,X_test1,y_train1,y_test1=sas(digits.data,digits.target)
cy_train1=ohe(y_train1); cy_test1=ohe(y_test1)
X_train1.shape,y_train1.shape,X_test1.shape,y_test1.shape
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
n=5; img=numpy.zeros((10*n,10*n))
for i in range(n): 
    for j in range(n): img[(10*i+1):(10*i+9),(10*j+1):(10*j+9)]=X_train1[i*n+j].reshape((8,8))
pylab.figure(figsize=(n,n)); pylab.imshow(img,cmap=pylab.cm.Blues)
pylab.title('Examples of 64-dimensional digits')
pylab.xticks([]); pylab.yticks([]); pylab.show()
print(y_train1[:n*n])
</script></div>
    <p></p> 
<h4>Dataset #2. MNIST</h4>
<div class="linked"><script type="text/x-sage">
path='https://s3.amazonaws.com/img-datasets/'; f='mnist.npz'
input_file=urllib.urlopen(path+f); output_file=open(f,'wb'); output_file.write(input_file.read()) 
input_file.close(); output_file.close(); mnist=numpy.load(f) 
[X_train2,y_train2,X_test2,y_test2]=[mnist[el]for el in ['x_train','y_train','x_test','y_test']]
cy_train2=ohe(y_train2); cy_test2=ohe(y_test2)
X_train2.shape,y_train2.shape,X_test2.shape,y_test2.shape
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
n=5; fig,ax=pylab.subplots(figsize=(n,n+1),nrows=n,ncols=n,sharex=True,sharey=True)
ax=ax.flatten(); [ax[i].imshow(X_train2[i],cmap=pylab.cm.Blues) for i in range(n*n)]
ax[0].set_xticks([]); ax[0].set_yticks([]); pylab.tight_layout(); pylab.gcf() 
ax[2].set_title('Examples of the 784-dimensional digits',fontsize=12); pylab.show()
print(y_train2[:n*n])
</script></div>
    <p></p>
<h4>Dataset #3. Synthetic - two digits</h4>
<div class="linked"><script type="text/x-sage">
img1=numpy.concatenate((X_train2[20],X_train2[30]),axis=1); zero1=numpy.zeros((14,56))
img2=numpy.concatenate((zero1,img1,zero1),axis=0)
pylab.figure(figsize=(3,4)); pylab.imshow(img2,cmap=pylab.cm.Blues)
pylab.title('Example of two concatenated\n784-dimensional digits')
pylab.xticks([]); pylab.yticks([]); pylab.show()
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
n1,n2=int(X_train2.shape[0]/2),int(X_test2.shape[0]/2); h1=random.randrange(0,28); h2=28-h1
X_train56=[numpy.concatenate((X_train2[:n1][i],X_train2[n1:][i]),axis=1) for i in range(n1)]
X_train56=numpy.array([numpy.concatenate((numpy.zeros((h1,56)),X_train56[i],numpy.zeros((h2,56))),axis=0) for i in range(n1)])
X_test56=[numpy.concatenate((X_test2[:n2][i],X_test2[n2:][i]),axis=1) for i in range(n2)]
X_test56=numpy.array([numpy.concatenate((numpy.zeros((h1,56)),X_test56[i],numpy.zeros((h2,56))),axis=0) for i in range(n2)])
y_train56=numpy.array([numpy.concatenate((y_train2.reshape(-1,1)[:n1][i],y_train2.reshape(-1,1)[n1:][i])) for i in range(n1)])
y_test56=numpy.array([numpy.concatenate((y_test2.reshape(-1,1)[:n2][i],y_test2.reshape(-1,1)[n2:][i])) for i in range(n2)])
k=20; pylab.imshow(X_test56[k],cmap=pylab.cm.Blues); pylab.title('Example of the third dataset')
pylab.show(); print(y_test56[k])
X_train56=X_train56.reshape(-1,1,56,56).astype('float32'); X_test56=X_test56.reshape(-1,1,56,56).astype('float32')
</script></div>
    <p></p>
<h4>Dataset #4. Synthetic - five digits with rotation</h4>
<div class="linked"><script type="text/x-sage">
X_train2r=numpy.array([random_rotate(image) for image in X_train2])
X_test2r=numpy.array([random_rotate(image) for image in X_test2])
print(y_test2[k]); pylab.imshow(X_test2r[k],cmap=pylab.cm.Blues)
pylab.title('Example of rotation')
pylab.xticks([]); pylab.yticks([]); pylab.show()
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
n=10000
X_train140,y_train140=concatenate(X_train2r[:n],y_train2.reshape(-1,1)[:n],28)
X_test140,y_test140=concatenate(X_test2r[:int(n/5)],y_test2.reshape(-1,1)[:int(n/5)],28)
del X_train2r,X_test2r
print(y_test140[k]); pylab.imshow(X_test140[k],cmap=pylab.cm.Blues)
pylab.title('Example of the final synthetic datasets')
pylab.xticks([]); pylab.yticks([]); pylab.show()
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
with h5py.File('Synthetic5Digits.h5','w') as f:
    f.create_dataset('X_train',data=X_train140); f.create_dataset('X_test',data=X_test140)
    f.create_dataset('y_train',data=y_train140); f.create_dataset('y_test',data=y_test140)
    f.close()
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
f=h5py.File('Synthetic5Digits.h5','r'); keys=list(f.keys()); print(keys)
X_test140=numpy.array(f[keys[0]]); X_train140=numpy.array(f[keys[1]])
y_test140=numpy.array(f[keys[2]]); y_train140=numpy.array(f[keys[3]])
cy_train140=numpy.empty([len(y_train140),5,11]).astype('int16')
for i in range(5): cy_train140[:,i]=ohe(y_train140[:,i].reshape(-1,1),categories=[list(range(11))])
cy_test140=numpy.empty([len(y_test140),5,11]).astype('int16')
for i in range(5): cy_test140[:,i]=ohe(y_test140[:,i].reshape(-1,1),categories=[list(range(11))])
print(y_test140[k]); print(cy_test140[k]); pylab.imshow(X_test140[k],cmap=pylab.cm.Blues)
pylab.title('Example of the synthetic dataset (h5)')
pylab.xticks([]); pylab.yticks([]); pylab.show()
</script></div>
    <p></p>
    <h2>Step 1: Design and Test a Model Architecture</h2>
In this project, we will design and implement a deep learning model that learns to recognize sequences of digits.<br/>
Also, we will train the model using synthetic data generated by concatenating character images from <i>notMNIST</i> or <i>MNIST</i>.<br/>
To produce a synthetic sequence of digits for testing, we can, for example, limit ourself to sequences up to five digits, and use five classifiers on top of the deep network.<br/>
We would have to incorporate an additional <i>blank</i> character to account for shorter number sequences.<br/>
There are various aspects to consider when thinking about this problem:<br/>
- The model can be derived from a convolutional network or a multi-layer perceptron classifier.<br/>
- We could experiment sharing or not the weights between the softmax classifiers.<br/>
- We can also use a recurrent network in your deep neural net to replace the classification layers and directly emit the sequence of digits one-at-a-time.<br/>
We can use the module <i>keras</i> or <i>tensorflow</i> or <i>sklearn</i> to implement the model.
    <h3>Examples of Models</h3>
Models N1 and N2 are classic examples, other models were designed for this project.
<h4>Model N1. Multi-Layer Perceptron</h4>
<div class="linked"><script type="text/x-sage">
class NeuralNetMLP(object):
    def __init__(self,n_output,n_features,n_hidden=30,l1=0.0,l2=0.0,epochs=500,eta=0.001, 
                 alpha=0.0,decrease_const=0.0,shuffle=True,minibatches=1,random_state=None):
        numpy.random.seed(random_state); self.epochs=epochs; self.shuffle=shuffle; self.minibatches=minibatches 
        self.n_output=n_output; self.n_features=n_features; self.n_hidden=n_hidden 
        self.l1=l1; self.l2=l2; self.eta=eta; self.alpha=alpha; self.decrease_const=decrease_const
        self.w1,self.w2=self._initialize_weights()
    def _encode_labels(self,y,k):
        onehot=numpy.zeros((k,y.shape[0]))
        for idx,val in enumerate(y): onehot[val,idx]=1.0
        return onehot
    def _initialize_weights(self):
        w1=numpy.random.uniform(-1.0,1.0,size=self.n_hidden*(self.n_features+1))      
        w2=numpy.random.uniform(-1.0,1.0,size=self.n_output*(self.n_hidden+1))
        return w1.reshape(self.n_hidden,self.n_features+1),w2.reshape(self.n_output,self.n_hidden+1)        
    def _sigmoid(self,z): return 1.0/(1.0+numpy.exp(-z))
    def _sigmoid_gradient(self,z): return self._sigmoid(z)*(1-self._sigmoid(z))
    def _add_bias_unit(self,X,how='column'):
        if how=='column': X_new=numpy.ones((X.shape[0],X.shape[1]+1)); X_new[:,1:]=X
        elif how=='row': X_new=numpy.ones((X.shape[0]+1,X.shape[1])); X_new[1:,:]=X
        else: raise AttributeError('`how` must be `column` or `row`')
        return X_new
    def _feedforward(self,X,w1,w2):
        a1=self._add_bias_unit(X,how='column'); z2=w1.dot(a1.T); a2=self._sigmoid(z2)
        a2=self._add_bias_unit(a2,how='row'); z3=w2.dot(a2); a3=self._sigmoid(z3)
        return a1,z2,a2,z3,a3
    def _L2_reg(self,lambda_,w1,w2): return (lambda_/2.0)*(numpy.sum(w1[:,1:]**2)+numpy.sum(w2[:,1:]**2))
    def _L1_reg(self,lambda_,w1,w2): return (lambda_/2.0)*(numpy.abs(w1[:,1:]).sum()+numpy.abs(w2[:,1:]).sum())
    def _get_cost(self,y_enc,output,w1,w2):
        term1=-y_enc*(numpy.log(output)); term2=(1-y_enc)*numpy.log(1-output)
        L1_term=self._L1_reg(self.l1,w1,w2); L2_term=self._L2_reg(self.l2,w1,w2)
        return numpy.sum(term1-term2)+L1_term+L2_term
    def _get_gradient(self,a1,a2,a3,z2,y_enc,w1,w2):
        sigma3=a3-y_enc; z2=self._add_bias_unit(z2,how='row')
        sigma2=w2.T.dot(sigma3)*self._sigmoid_gradient(z2)
        sigma2=sigma2[1:,:]; grad1=sigma2.dot(a1); grad2=sigma3.dot(a2.T)
        grad1[:,1:]+=(w1[:,1:]*(self.l1+self.l2)); grad2[:,1:]+=(w2[:,1:]*(self.l1+self.l2))
        return grad1,grad2
    def predict(self,X):
        a1,z2,a2,z3,a3=self._feedforward(X, self.w1,self.w2)
        return numpy.argmax(z3,axis=0)
    def fit(self,X,y,print_progress=False):
        self.cost_=[]; X_data,y_data=X.copy(),y.copy(); y_enc=self._encode_labels(y,self.n_output)
        delta_w1_prev=numpy.zeros(self.w1.shape); delta_w2_prev=numpy.zeros(self.w2.shape)
        for i in range(self.epochs):
            self.eta/=(1+self.decrease_const*i)
            if (print_progress and i%100==0): sys.stderr.write('\nEpoch: %d/%d'%(i+1,self.epochs)); sys.stderr.flush()
            if self.shuffle:
                idx=numpy.random.permutation(y_data.shape[0])
                X_data,y_enc=X_data[idx],y_enc[:,idx]
            mini=numpy.array_split(range(y_data.shape[0]),self.minibatches)           
            for idx in mini:
                a1,z2,a2,z3,a3=self._feedforward(X_data[idx],self.w1,self.w2)
                cost=self._get_cost(y_enc=y_enc[:,idx],output=a3,w1=self.w1,w2=self.w2)                
                self.cost_.append(cost)
                grad1,grad2=self._get_gradient(a1=a1,a2=a2,a3=a3,z2=z2,y_enc=y_enc[:,idx],w1=self.w1,w2=self.w2)
                delta_w1,delta_w2=self.eta*grad1,self.eta*grad2
                self.w1-=(delta_w1+(self.alpha*delta_w1_prev)); self.w2-=(delta_w2+(self.alpha*delta_w2_prev))
                delta_w1_prev,delta_w2_prev=delta_w1,delta_w2
        return self
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
print('Dataset N1. Rows: %d, columns: %d'%(X_train1.shape[0],X_train1.shape[1]))
nn1=NeuralNetMLP(n_output=10,n_features=X_train1.shape[1],n_hidden=128,l2=0.1,l1=0.0,epochs=800, 
                 eta=0.01,alpha=0.01,decrease_const=0.001,shuffle=True,minibatches=128,random_state=0)
nn1.fit(X_train1,y_train1,print_progress=True)
y_train1_pred=nn1.predict(X_train1); y_test1_pred=nn1.predict(X_test1)
nn1_train1_accuracy=1.*numpy.sum(y_train1==y_train1_pred)/X_train1.shape[0]
nn1_test1_accuracy=1.*numpy.sum(y_test1==y_test1_pred)/X_test1.shape[0]
print('Train accuracy: %.2f%%'%(nn1_train1_accuracy*100))
print('Test accuracy: %.2f%%'%(nn1_test1_accuracy*100))
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
n=5000
print('Dataset N2. Rows: %d, columns: %d'%(n,X_train2.shape[1]*X_train2.shape[2]))
nn2=NeuralNetMLP(n_output=10,n_features=784,n_hidden=32,l2=0.01,l1=0.0,epochs=800, 
                 eta=0.001,alpha=0.001,decrease_const=0.00001,shuffle=True,minibatches=8,random_state=1)
nn2.fit(X_train2[:n].reshape(-1,784),y_train2[:n],print_progress=True)
y_train2_pred=nn2.predict(X_train2.reshape(-1,784)); y_test2_pred=nn2.predict(X_test2.reshape(-1,784))
nn1_train2_accuracy=1.*numpy.sum(y_train2==y_train2_pred)/X_train2.shape[0]
nn1_test2_accuracy=1.*numpy.sum(y_test2==y_test2_pred)/X_test2.shape[0]
print('Train accuracy: %.2f%%'%(nn1_train2_accuracy*100))
print('Test accuracy: %.2f%%'%(nn1_test2_accuracy*100))
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
clf=MLPClassifier(hidden_layer_sizes=(272,),max_iter=5,solver='adam',verbose=0,random_state=1,learning_rate_init=.01)
clf.fit(X_train2.reshape(-1,784),y_train2); print(clf.score(X_test2.reshape(-1,784),y_test2))
y_test2_predictions=clf.predict(X_test2.reshape(-1,784))
pylab.figure(figsize=(12,5)); pylab.scatter(range(100),y_test2[:100],color='#b8e2fc',s=100)
pylab.scatter(range(100),y_test2_predictions[:100],color='#191970',s=25); pylab.show()
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
    <h3>Additional Code Cell</h3>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p> 
  </body>
</html>