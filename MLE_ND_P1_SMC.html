<!DOCTYPE HTML>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=0.9*device-width">
    <title>MLE_ND_P1_SMC</title>
    <script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script>
    <script>$(function () {
    sagecell.makeSagecell({inputLocation:'div.linked',linked:true,evalButtonText:'Run Linked Cells'});  
    sagecell.makeSagecell({inputLocation:'div.sage',evalButtonText:'Run'});
    });
    </script>
  </head>
  <style>
  @import url('https://fonts.googleapis.com/css?family=Orbitron|Roboto');
  body {background-color:aliceblue;}; a {color:#4876ff; font-family:'Roboto';} 
  h1 {color:#348ABD; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;} 
  h2,h3 {color:slategray; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;}
  h4 {color:#348ABD; font-family:'Roboto';}
  .sagecell .CodeMirror-scroll {min-height:3em; max-height:48em;}
  </style>  
  <body>
    <h1>Machine Learning Engineer Nanodegree</h1>
    <h2>Model Evaluation and Validation</h2>
    <h1>&#x1F4D1; &nbsp;P1: Predicting Boston Housing Prices</h1>
    <h2>Getting Started</h2>
    <h3>Dataset</h3>
In this project, we will evaluate the performance and predictive power of a model that has been trained and tested on data collected from homes in suburbs of Boston, Massachusetts.<br/>A model trained on this data that is seen as a good fit could then be used to make certain predictions about a home â€” in particular, its monetary value.<br/>This model would prove to be invaluable for someone like a real estate agent who could make use of such information on a daily basis.<br/>
<i>Origin</i>: This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.<br/>
<i>Creators</i>: Harrison, D. and Rubinfeld, D.L.<br/>
<i>Data Set Information</i>: Concerns housing values in suburbs of Boston.<br/>
<i>Attribute Information</i>:<br/>
CRIM: per capita crime rate by town<br/>
ZN: proportion of residential land zoned for lots over 25,000 sq.ft.<br/>
INDUS: proportion of non-retail business acres per town<br/>
CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)<br/>
NOX: nitric oxides concentration (parts per 10 million)<br/>
RM: average number of rooms per dwelling<br/>
AGE: proportion of owner-occupied units built prior to 1940<br/>
DIS: weighted distances to five Boston employment centres<br/>
RAD: index of accessibility to radial highways<br/>
TAX: full-value property-tax rate per 10,000 USD<br/>
PTRATIO: pupil-teacher ratio by town<br/>
B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town<br/>
LSTAT: % lower status of the population<br/>
MEDV: Median value of owner-occupied homes in 1000 USD<br/>
The Boston housing data was collected in 1978 and each of the 506 entries represents aggregated data about 14 features for homes from various suburbs.<br/>
For the purposes of this project, the following preprocessing steps have been made to the dataset:<br/>
16 data points have an 'MEDV' value of 50.0. These data points likely contain missing or censored values and have been removed.<br/>
1 data point has an 'RM' value of 8.78. This data point can be considered an outlier and has been removed.<br/>
The features 'RM', 'LSTAT', 'PTRATIO', and 'MEDV' are essential. The remaining non-relevant features have been excluded.<br/>
The feature 'MEDV' has been multiplicatively scaled to account for 35 years of market inflation.
      <h3>Resources</h3>
<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/">&#x1F578;UCI Housing Dataset&nbsp;</a>
<a href="http://archive.ics.uci.edu/ml/datasets.php">&#x1F578;UCI Machine Learning Repository&nbsp;</a>
<a href="https://scikit-learn.org/stable/index.html">&#x1F578;scikit-learn. Machine Learning in Python&nbsp;</a>
<a href="http://seaborn.pydata.org/index.html">&#x1F578;seaborn: statistical data visualization&nbsp;</a>
      <h3>Code Library</h3>      
<div class="linked"><script type="text/x-sage">
import numpy,pandas,pylab,seaborn; pylab.style.use('seaborn-whitegrid')
from sklearn.model_selection import train_test_split,ShuffleSplit,GridSearchCV
from sklearn.metrics import mean_squared_error,r2_score,make_scorer
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import learning_curve as curves
from sklearn import datasets
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
# https://github.com/udacity/machine-learning/blob/master/projects/boston_housing/visuals.py
def ModelLearning(X, y):
    cv=ShuffleSplit(X.shape[0],n_iter=10,test_size=0.2,random_state=1)
    train_sizes=numpy.rint(numpy.linspace(1,X.shape[0]*0.8-1,9)).astype(int)
    fig=pylab.figure(figsize=(12,10))
    for k,depth in enumerate([1,3,6,10]):
        regressor=DecisionTreeRegressor(max_depth=depth)
        sizes,train_scores,test_scores=curves.learning_curve(regressor,X,y,cv=cv,train_sizes=train_sizes,scoring='r2')
        train_std=numpy.std(train_scores,axis=1); train_mean=numpy.mean(train_scores,axis=1)
        test_std=numpy.std(test_scores,axis=1); test_mean=numpy.mean(test_scores,axis=1)
        ax=fig.add_subplot(2,2,k+1)
        ax.plot(sizes,train_mean,'o-',color='#E24A33',label='Training Score')
        ax.plot(sizes,test_mean,'o-',color='#348ABD',label='Testing Score')
        ax.fill_between(sizes,train_mean-train_std,train_mean+train_std,alpha=0.15,color='#E24A33')
        ax.fill_between(sizes,test_mean-test_std,test_mean+test_std,alpha=0.15,color = '#348ABD')
        ax.set_title('max_depth=%s'%(depth)); ax.set_xlabel('Number of Training Points')
        ax.set_ylabel('Score'); ax.set_xlim([0,X.shape[0]*0.8]); ax.set_ylim([-0.05,1.05])
    ax.legend(bbox_to_anchor=(1.05,2.05),loc='lower left',borderaxespad=0.)
    fig.suptitle('Decision Tree Regressor Learning Performances',fontsize=16,y=1.03)
    fig.tight_layout(); pylab.show()
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
def ModelComplexity(X, y):
    cv=ShuffleSplit(X.shape[0],n_iter=10,test_size=0.2,random_state=0)
    max_depth=numpy.arange(1,11)
    train_scores,test_scores=curves\
    .validation_curve(DecisionTreeRegressor(),X,y,param_name="max_depth",param_range=max_depth,cv=cv,scoring='r2')
    train_mean=numpy.mean(train_scores,axis=1); train_std=numpy.std(train_scores,axis=1)
    test_mean=numpy.mean(test_scores,axis=1); test_std=numpy.std(test_scores,axis=1)
    pylab.figure(figsize=(12,5)); pylab.title('Decision Tree Regressor Complexity Performance')
    pylab.plot(max_depth, train_mean,'o-',color='#E24A33',label='Training Score')
    pylab.plot(max_depth,test_mean,'o-',color='#348ABD',label='Validation Score')
    pylab.fill_between(max_depth,train_mean-train_std,train_mean+train_std,alpha=0.15,color='#E24A33')
    pylab.fill_between(max_depth,test_mean-test_std,test_mean+test_std,alpha=0.15,color='#348ABD')
    pylab.legend(loc='lower right'); pylab.xlabel('Maximum Depth'); pylab.ylabel('Score')
    pylab.ylim([-0.05,1.05]); pylab.show()
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
def PredictTrials(X,y,fitter,data):
    prices=[]
    for k in range(10):
        X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=k)
        reg=fitter(X_train,y_train).best_estimator_
        pred=reg.predict([data[0]])[0]
        prices.append(pred)
        print ("Trial {}: ${:,.2f}".format(k+1,pred))
    print ("\nRange in prices: ${:,.2f}".format(max(prices)-min(prices)))
</script></div>
    <p></p>
      <h2>Data Exploration</h2>
In this first section of this project, we will make a cursory investigation about the Boston housing data and provide the observations.<br/>
Familiarizing ourself with the data through an explorative process is a fundamental practice to help us better understand and justify the results.<br/>
Since the main goal of this project is to construct a working model which has the capability of predicting the value of houses,<br/>we will need to separate the dataset into the <b>features</b> and the <b>target</b> variable.<br/>
The features, 'RM', 'LSTAT', and 'PTRATIO', give us quantitative information about each data point.<br/>
The <b>target</b> variable, 'MEDV', will be the variable we seek to predict. These are stored in features and prices, respectively.    
<div class="linked"><script type="text/x-sage">
path='https://raw.githubusercontent.com/OlgaBelitskaya/machine_learning_engineer_nd009/master/Machine_Learning_Engineer_ND_P1/'
data=pandas.read_csv(path+'housing.csv'); prices=data['MEDV']; features=data.drop('MEDV',axis=1)
print ("Boston housing dataset has {} data points with {} variables each.".format(*data.shape))
data.describe().T
</script></div>
    <p></p>
    <h3>Implementation: Calculate Statistics</h3>
For the very first coding implementation, we will calculate descriptive statistics about the Boston housing prices.<br/>
Since numpy has already been imported, this library is used to perform the necessary calculations.<br/> 
These statistics will be extremely important later on to analyze various prediction results from the constructed model.<br/> 
In the code cell below, we will need to implement the following:<br/>
Calculate the minimum, maximum, mean, median, and standard deviation of 'MEDV', which is stored in prices.<br/>
Store each calculation in their respective variable.<br/>
<div class="linked"><script type="text/x-sage">
print ("Boston Housing Dataset Statistics: \n")
print ("Number of houses = ", len(prices))
print ("Number of features = ", len(list(features.keys())))
print ("Minimum house price = ", numpy.min(prices))
print ("Maximum house price = ", numpy.max(prices))
print ("Mean house price = ", "%.2f" % numpy.mean(prices))
print ("Median house price = ", "%.2f" % numpy.median(prices))
print ("Standard deviation of house prices =", "%.2f" % numpy.std(prices))
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
pylab.figure(figsize=(12,5)); seaborn.distplot(prices,color='#348ABD')
pylab.xlabel("Prices"); pylab.title('Boston Housing Data'); pylab.show()
</script></div>
    <p></p>
    <h3>Question 1 - Feature Observation</h3>
As a reminder, we are using three features from the Boston housing dataset: 'RM', 'LSTAT', and 'PTRATIO'.<br/>
For each data point (neighborhood):<br/>
'RM' is the average number of rooms among homes in the neighborhood.<br/>
'LSTAT' is the percentage of homeowners in the neighborhood considered "lower class" (working poor).<br/>
'PTRATIO' is the ratio of students to teachers in primary and secondary schools in the neighborhood.<br/>
<i>Using your intuition, for each of the three features above, do you think that an increase in the value of that feature</i> 
      <br/><i>would lead to an <b>increase</b> in the value of 'MEDV' or a <b>decrease</b> in the value of 'MEDV'? Justify your answer for each.</i>
    <h3>Answer 1</h3>
My assumptions could be:<br/>
'RM': shows the level of home comfort, its increase would lead to the increase in the value of 'MEDV';<br/>
'PTRATIO': shows the level of educational resources, its increase would lead to the decrease in the value of 'MEDV';<br/>
'LSTAT': indicates the level of social environment comfort, its increase would lead to the decrease in the value of 'MEDV'.<br/>
I have made the correlation list for all features in the original dataset to confirm the assumptions and created plots for the transformed features<br/>with the least squares regression fitted line and the hexagon-aggregated 2D histograms as an example to illustrate the trends.
<div class="linked"><script type="text/x-sage">
boston_data=datasets.load_boston(); boston_df=pandas.DataFrame(boston_data.data,columns=boston_data.feature_names)
boston_df['MEDV']=boston_data.target; pearson=boston_df.corr(method='pearson')
corr_with_prices=pearson['MEDV'].loc[:'LSTAT'] 
corr_with_prices[abs(corr_with_prices).sort_values(ascending=False).index]
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
def lstsq_plot(feature):
    pylab.figure(figsize=(12,5)); x,y=features[feature],prices
    x1=numpy.array([[v,1] for v in x]); y1=numpy.array([[v,1] for v in y])
    (slope,bias),_,_,_=numpy.linalg.lstsq(x1,y1,rcond=None); y_lin=x1*slope+bias
    pylab.scatter(x1,y1,color='#348ABD',marker='*',facecolors='none',label='Original data')
    pylab.scatter(x1,y_lin,color='#104E8B',marker='v',facecolors='none',label='Fitted line')
    pylab.xlabel('Feature %s'%feature); pylab.ylabel('Prices')
    pylab.title('Boston Housing Data'); pylab.legend(); pylab.show()
seaborn.jointplot(features['RM'],prices,kind='hex',joint_kws={'alpha':0.8},height=5,color='#348ABD')
pylab.show(); lstsq_plot('RM')
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
seaborn.jointplot(features['PTRATIO'],prices,kind='hex',joint_kws={'alpha':0.8},height=5,color='#348ABD')
pylab.show(); lstsq_plot('PTRATIO')
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
seaborn.jointplot(features['LSTAT'],prices,kind='hex',joint_kws={'alpha':0.8},height=5,color='#348ABD')
pylab.show(); lstsq_plot('LSTAT')
</script></div>
    <p></p>
    <h2>Developing a Model</h2>
In this section of the project, we will develop the tools and techniques necessary for a model to make a prediction.<br/>
Being able to make accurate evaluations of each model's performance through the use of these tools and techniques helps to greatly reinforce the confidence in the predictions.
    <h3>Implementation: Define a Performance Metric</h3>
It is difficult to measure the quality of a given model without quantifying its performance over training and testing.<br/>
This is typically done using some type of performance metric, whether it is through calculating some type of error, the goodness of fit, or some other useful measurement.<br/>
For this project, we will be calculating the coefficient of determination, <i>$R^2$</i>, to quantify the model's performance.<br/> 
The coefficient of determination for a model is a useful statistic in regression analysis, as it often describes how "good" that model is at making predictions.<br/>
The values for <i>$R^2$</i> range from 0 to 1, which captures the percentage of squared correlation between the predicted and actual values of the target variable.<br/>
A model with an <i>$R^2$</i> of 0 is no better than a model that always predicts the mean of the target variable, whereas a model with an <i>$R^2$</i> of 1 perfectly predicts the target variable.<br/> 
Any value between 0 and 1 indicates what percentage of the target variable, using this model, can be explained by the features.<br/>
A model can be given a negative <i>$R^2$</i> as well, which indicates that the model is arbitrarily worse than one that always predicts the mean of the target variable.<br/>
For the <i>performance_metric()</i> function, we will need to implement the following:<br/>
- use <i>r2_score</i> from <i>sklearn.metrics</i> to perform a performance calculation between <i>y_true</i> and <i>y_predict</i>;<br/>
- assign the performance score to the score variable.
<div class="linked"><script type="text/x-sage">
def performance_metric(y_true, y_predict): return r2_score(y_true,y_predict)
</script></div>
    <p></p>
    <h3>Question 2 - Goodness of Fit</h3>
Assume that a dataset contains five data points and a model made the following predictions for the target variable:
$\begin{tabular}{ll} True Value & Prediction \\ 3.0 & 2.5 \\ -0.5 & 0.0 \\ 2.0 & 2.1 \\ 7.0 & 7.8 \\ 4.2 & 5.3 \\ \end{tabular}$
<i>Would you consider this model to have successfully captured the variation of the target variable? Why or why not?</i>
<div class="linked"><script type="text/x-sage">
display(table([['True Value','Prediction'],[3.0,2.5],[-0.5,0.0],[2.0,2.1],[7.0,7.8],[4.2,5.3]]))
score=performance_metric([3,-0.5,2,7,4.2],[2.5,0.0,2.1,7.8,5.3])
print ("Model has a coefficient of determination, R^2, of {:.3f}.".format(score))
</script></div>
<p></p>
    <h3>Answer 2</h3>
<i>$R^2$</i> indicates the performance of the model between the predicted and observed values of the target variable very well (especially for linear regression).<br/>
It is close to 1 and seems that the model predicts quite accurately. But we must be extremely careful with the conclusions in the case of a small amount of data.<br/>
In addition, it is unknown whether it is a linear model and how many variables were needed to build it (details are given about the target variable only).<br/>
Therefore, it can be stated only that a reasonably accurate prediction is possible to build in this case, but it is desirable to have more data.
    <h3>Implementation: Shuffle and Split Data</h3>
The next implementation requires that we take the Boston housing dataset and split the data into training and testing subsets.<br/>Typically, the data is splitted and shuffled into a random order when creating the training and testing subsets to remove any bias in the ordering of the dataset.
<div class="linked"><script type="text/x-sage">
X_train,X_test,y_train,y_test=train_test_split(features,prices,test_size=0.2,random_state=1)
print ("Training and testing split was successful.")
</script></div>
    <p></p>
    <h3>Question 3 - Training and Testing</h3>
What is the benefit to splitting a dataset into some ratio of training and testing subsets for a learning algorithm?
    <h3>Answer 3</h3>
We should not only build a predictive model but also assess the quality of it.<br/> 
Naturally, we would like to know how our predictions will be relatively close to the actual outcomes.<br/>
We usually split the data into training and testing subsets exactly for this goal.<br/> 
The training set is used to choose the most effective parameters for given models.<br/>
But what kind of model we should apply and how the concrete model works we can evaluate with the test set.<br/>
It helps to avoid overfitting, i.e. the cases when the built model <br/>
- will fit extremely well for the training sets and <br/>
- will not work with real data because of catching non-existing trends.
    <h2>Analyzing Model Performance</h2>
      
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
  </body>
</html>