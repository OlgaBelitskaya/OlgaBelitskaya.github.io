<!DOCTYPE HTML>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=0.9*device-width">
    <title>MLE_ND_P1_SMC</title>
    <script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script>
    <script>$(function () {
    sagecell.makeSagecell({inputLocation:'div.linked',linked:true,evalButtonText:'Run Linked Cells'});  
    sagecell.makeSagecell({inputLocation:'div.sage',evalButtonText:'Run'});
    });
    </script>
  </head>
  <style>
  @import url('https://fonts.googleapis.com/css?family=Orbitron|Roboto');
  body {background-color:aliceblue;}; a {color:#4876ff; font-family:'Roboto';} 
  h1 {color:#348ABD; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;} 
  h2,h3 {color:slategray; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;}
  h4 {color:#348ABD; font-family:'Roboto';}
  .sagecell .CodeMirror-scroll {min-height:3em; max-height:48em;}
  </style>  
  <body>
    <h1>Machine Learning Engineer Nanodegree</h1>
    <h2>Model Evaluation and Validation</h2>
    <h1>&#x1F4D1; &nbsp;P1: Predicting Boston Housing Prices</h1>
    <h2>Getting Started</h2>
    <h3>Dataset</h3>
In this project, we will evaluate the performance and predictive power of a model that has been trained and tested on data collected from homes in suburbs of Boston, Massachusetts.<br/>A model trained on this data that is seen as a good fit could then be used to make certain predictions about a home â€” in particular, its monetary value.<br/>This model would prove to be invaluable for someone like a real estate agent who could make use of such information on a daily basis.<br/>
<i>Origin</i>: This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.<br/>
<i>Creators</i>: Harrison, D. and Rubinfeld, D.L.<br/>
<i>Data Set Information</i>: Concerns housing values in suburbs of Boston.<br/>
<i>Attribute Information</i>:<br/>
CRIM: per capita crime rate by town<br/>
ZN: proportion of residential land zoned for lots over 25,000 sq.ft.<br/>
INDUS: proportion of non-retail business acres per town<br/>
CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)<br/>
NOX: nitric oxides concentration (parts per 10 million)<br/>
RM: average number of rooms per dwelling<br/>
AGE: proportion of owner-occupied units built prior to 1940<br/>
DIS: weighted distances to five Boston employment centres<br/>
RAD: index of accessibility to radial highways<br/>
TAX: full-value property-tax rate per 10,000 USD<br/>
PTRATIO: pupil-teacher ratio by town<br/>
B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town<br/>
LSTAT: % lower status of the population<br/>
MEDV: Median value of owner-occupied homes in 1000 USD<br/>
The Boston housing data was collected in 1978 and each of the 506 entries represents aggregated data about 14 features for homes from various suburbs.<br/>
For the purposes of this project, the following preprocessing steps have been made to the dataset:<br/>
16 data points have an 'MEDV' value of 50.0. These data points likely contain missing or censored values and have been removed.<br/>
1 data point has an 'RM' value of 8.78. This data point can be considered an outlier and has been removed.<br/>
The features 'RM', 'LSTAT', 'PTRATIO', and 'MEDV' are essential. The remaining non-relevant features have been excluded.<br/>
The feature 'MEDV' has been multiplicatively scaled to account for 35 years of market inflation.
      <h3>Resources</h3>
<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/">&#x1F578;UCI Housing Dataset&nbsp;</a>
<a href="http://archive.ics.uci.edu/ml/datasets.php">&#x1F578;UCI Machine Learning Repository&nbsp;</a>
<a href="https://scikit-learn.org/stable/index.html">&#x1F578;scikit-learn. Machine Learning in Python&nbsp;</a>
<a href="http://seaborn.pydata.org/index.html">&#x1F578;seaborn: statistical data visualization&nbsp;</a>
      <h3>Code Library</h3>      
<div class="linked"><script type="text/x-sage">
import numpy,pandas,pylab,seaborn
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.metrics import mean_squared_error,r2_score,make_scorer
from sklearn.tree import DecisionTreeRegressor
import sklearn.learning_curve as curves
from sklearn.cross_validation import ShuffleSplit
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
# https://github.com/udacity/machine-learning/blob/master/projects/boston_housing/visuals.py
def ModelLearning(X, y):
    cv=ShuffleSplit(X.shape[0],n_iter=10,test_size=0.2,random_state=1)
    train_sizes=numpy.rint(numpy.linspace(1,X.shape[0]*0.8-1,9)).astype(int)
    fig=pylab.figure(figsize=(12,10)); pylab.style.use('seaborn-whitegrid')
    for k,depth in enumerate([1,3,6,10]):
        regressor=DecisionTreeRegressor(max_depth=depth)
        sizes,train_scores,test_scores=curves.learning_curve(regressor,X,y,cv=cv,train_sizes=train_sizes,scoring='r2')
        train_std=numpy.std(train_scores,axis=1); train_mean=numpy.mean(train_scores,axis=1)
        test_std=numpy.std(test_scores,axis=1); test_mean=numpy.mean(test_scores,axis=1)
        ax=fig.add_subplot(2,2,k+1)
        ax.plot(sizes,train_mean,'o-',color='#E24A33',label='Training Score')
        ax.plot(sizes,test_mean,'o-',color='#348ABD',label='Testing Score')
        ax.fill_between(sizes,train_mean-train_std,train_mean+train_std,alpha=0.15,color='#E24A33')
        ax.fill_between(sizes,test_mean-test_std,test_mean+test_std,alpha=0.15,color = '#348ABD')
        ax.set_title('max_depth=%s'%(depth)); ax.set_xlabel('Number of Training Points')
        ax.set_ylabel('Score'); ax.set_xlim([0,X.shape[0]*0.8]); ax.set_ylim([-0.05,1.05])
    ax.legend(bbox_to_anchor=(1.05,2.05),loc='lower left',borderaxespad=0.)
    fig.suptitle('Decision Tree Regressor Learning Performances',fontsize=16,y=1.03)
    fig.tight_layout(); pylab.show()
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
def ModelComplexity(X, y):
    cv=ShuffleSplit(X.shape[0],n_iter=10,test_size=0.2,random_state=0)
    max_depth=numpy.arange(1,11)
    train_scores,test_scores=curves\
    .validation_curve(DecisionTreeRegressor(),X,y,param_name="max_depth",param_range=max_depth,cv=cv,scoring='r2')
    train_mean=numpy.mean(train_scores,axis=1); train_std=numpy.std(train_scores,axis=1)
    test_mean=numpy.mean(test_scores,axis=1); test_std=numpy.std(test_scores,axis=1)
    pylab.figure(figsize=(12,5)); pylab.title('Decision Tree Regressor Complexity Performance')
    pylab.plot(max_depth, train_mean,'o-',color='#E24A33',label='Training Score')
    pylab.plot(max_depth,test_mean,'o-',color='#348ABD',label='Validation Score')
    pylab.fill_between(max_depth,train_mean-train_std,train_mean+train_std,alpha=0.15,color='#E24A33')
    pylab.fill_between(max_depth,test_mean-test_std,test_mean+test_std,alpha=0.15,color='#348ABD')
    pylab.legend(loc='lower right'); pylab.xlabel('Maximum Depth'); pylab.ylabel('Score')
    pylab.ylim([-0.05,1.05]); pylab.show()
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
  </body>
</html>