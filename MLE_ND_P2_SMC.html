<!DOCTYPE HTML>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=0.9*device-width">
    <title>MLE_ND_P2_SMC</title>
    <script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script>
    <script>$(function () {
    sagecell.makeSagecell({inputLocation:'div.linked',linked:true,evalButtonText:'Run Linked Cells'});  
    sagecell.makeSagecell({inputLocation:'div.sage',evalButtonText:'Run'});
    });
    </script>
  </head>
  <style>
  @import url('https://fonts.googleapis.com/css?family=Orbitron|Roboto');
  body {background-color:mintcream;}; a,p {color:#00a050; font-family:'Roboto';} 
  h1 {color:#00A0A0; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;} 
  h2,h3 {color:slategray; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;}
  h4 {color:#00A0A0; font-family:'Roboto';}
  .sagecell .CodeMirror-scroll {min-height:3em; max-height:48em;}
  </style>  
  <body>
    <h1>Machine Learning Engineer Nanodegree</h1>
    <h2>Supervised Learning</h2>
    <h1>&#x1F4D1; &nbsp;P2: Finding Donors for CharityML</h1>
    <h2>Getting Started</h2>
    <h3>Data</h3>
In this project, we will employ several supervised algorithms of your choice to accurately model individuals' income using data collected from the 1994 U.S. Census.<br/>
We will then choose the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data.<br/>
The goal with this implementation is to construct a model that accurately predicts whether an individual makes more than $50,000.<br/>
This sort of task can arise in a non-profit setting, where organizations survive on donations.<br/>
Understanding an individual's income can help a non-profit better understand how large of a donation to request, or whether or not they should reach out to begin with.<br/>
While it can be difficult to determine an individual's general income bracket directly from public sources, we can (as we will see) infer this value from other publically available features.<br/>
The dataset for this project originates from the <a href="https://archive.ics.uci.edu/ml/datasets/Census+Income">&#x1F578;UCI Machine Learning Repository.</a><br/>
The datset was donated by Ron Kohavi and Barry Becker, after being published in the article "Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid".You can find the article by Ron Kohavi <a href="https://www.aaai.org/Papers/KDD/1996/KDD96-033.pdf">&#x1F578;online.</a><br/>
The data we investigate here consists of small changes to the original dataset, such as removing the <i>fnlwgt</i> feature and records with missing or ill-formatted entries.
      <h3>Resources</h3>
<a href="http://archive.ics.uci.edu/ml/datasets.php">&#x1F578;UCI Machine Learning Repository&nbsp;</a>
<a href="https://scikit-learn.org/stable/index.html">&#x1F578;scikit-learn. Machine Learning in Python&nbsp;</a>
<a href="http://seaborn.pydata.org/index.html">&#x1F578;seaborn: statistical data visualization&nbsp;</a><br/>
<a href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/">&#x1F578;A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning</a>
<a href="https://www.youtube.com/watch?v=9wn1f-30_ZY">&#x1F578;Gradient Boosting Method and Random Forest</a>
<a href="https://www.is.uni-freiburg.de/ressourcen/business-analytics/10_ensemblelearning.pdf">&#x1F578;Data Mining: Ensemble Learning</a>
      <h3>Code Library</h3> 
<div class="linked"><script type="text/x-sage">
import numpy,pandas,pylab,seaborn,time; pylab.style.use('seaborn-whitegrid')
import warnings; from sklearn.exceptions import DataConversionWarning
for el in [FutureWarning,DataConversionWarning]: warnings.filterwarnings("ignore",category=el)
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.metrics import f1_score,accuracy_score,fbeta_score,confusion_matrix,make_scorer
from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier,RandomForestClassifier
from sklearn.svm import LinearSVC; from sklearn.linear_model import SGDClassifier
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
# https://github.com/udacity/machine-learning/blob/master/projects/finding_donors/visuals.py
def distribution(data,transformed=False):
    fig=pylab.figure(figsize=(12,5));
    for i,feature in enumerate(['capital-gain','capital-loss']):
        ax=fig.add_subplot(1,2,i+1); ax.hist(data[feature],bins=30,color='#00A0A0')
        ax.set_title("'%s' Feature Distribution"%(feature),fontsize=14)
        ax.set_xlabel("Value"); ax.set_ylabel("Number of Records"); ax.set_ylim((0,2000)) 
        ax.set_yticks([0,500,1000,1500,2000]); ax.set_yticklabels([0,500,1000,1500,">2000"])
    if transformed:
        fig.suptitle("Log-transformed Distributions of Continuous Census Data Features",fontsize=16,y=1.03)
    else:
        fig.suptitle("Skewed Distributions of Continuous Census Data Features",fontsize=16,y=1.03)
    fig.tight_layout(); pylab.show()
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
def evaluate(results,accuracy,f1):
    fig,ax=pylab.subplots(2,3,figsize=(12,5))
    bar_width=0.3; colors=['#A00000','#00A0A0','#00A000']
    for k,learner in enumerate(results.keys()):
        for j,metric in enumerate(['train_time','acc_train','f_train','pred_time','acc_test','f_test']):
            for i in np.arange(3):
                ax[int(j/3),j%3].bar(i+k*bar_width,results[learner][i][metric],width=bar_width,color=colors[k])
                ax[int(j/3),j%3].set_xticks([0.45,1.45,2.45]); ax[int(j/3),j%3].set_xticklabels(["1%","10%","100%"])
                ax[int(j/3),j%3].set_xlabel("Training Set Size"); ax[int(j/3),j%3].set_xlim((-0.1,3.0))
    ax[0,0].set_ylabel("Time (in seconds)"); ax[0,1].set_ylabel("Accuracy Score"); ax[0,2].set_ylabel("F-score")
    ax[1,0].set_ylabel("Time (in seconds)"); ax[1,1].set_ylabel("Accuracy Score"); ax[1,2].set_ylabel("F-score")
    ax[0,0].set_title("Model Training"); ax[0,1].set_title("Accuracy Score on Training Subset")
    ax[0,2].set_title("F-score on Training Subset"); ax[1,0].set_title("Model Predicting")
    ax[1,1].set_title("Accuracy Score on Testing Set"); ax[1,2].set_title("F-score on Testing Set")
    ax[0,1].axhline(y=accuracy,xmin=-0.1,xmax=3.0,linewidth=1,color='k',linestyle='dashed')
    ax[1,1].axhline(y=accuracy,xmin=-0.1,xmax=3.0,linewidth=1,color='k',linestyle='dashed')
    ax[0,2].axhline(y=f1,xmin=-0.1,xmax=3.0,linewidth=1,color='k',linestyle='dashed')
    ax[1,2].axhline(y=f1,xmin=-0.1,xmax=3.0,linewidth=1,color='k',linestyle='dashed')
    ax[0,1].set_ylim((0,1)); ax[0,2].set_ylim((0,1)); ax[1,1].set_ylim((0,1)); ax[1,2].set_ylim((0,1))
    patches = []
    for i,learner in enumerate(results.keys()): patches.append(mpatches.Patch(color=colors[i],label=learner))
    pylab.legend(handles=patches,bbox_to_anchor=(-0.8,2.53),loc='upper center',borderaxespad=0.,ncol=3,fontsize='x-large')
    pylab.suptitle("Performance Metrics for Three Supervised Learning Models",fontsize=16,y=1.15)
    pylab.tight_layout(); pylab.show()
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
def feature_plot(importances,X_train,y_train):
    indices=numpy.argsort(importances)[::-1]; columns=X_train.columns.values[indices[:5]]
    values=importances[indices][:5]; fig=pylab.figure(figsize=(12,5))
    pylab.title("Normalized Weights for First Five Most Predictive Features",fontsize=16)
    pylab.bar(numpy.arange(5),values,width=0.6,align="center",color='#00A000',label="Feature Weight")
    pylab.bar(numpy.arange(5)-0.3,numpy.cumsum(values),width=0.2,align="center",color='#00A0A0',label="Cumulative Feature Weight")
    pylab.xticks(numpy.arange(5),columns); pylab.xlim((-0.5,4.5))
    pylab.ylabel("Weight",fontsize=12); pylab.xlabel("Feature",fontsize=12)   
    pylab.legend(loc='upper center'); pylab.tight_layout(); pylab.show()  
</script></div>
    <p></p>
    <h2>Exploring the Data</h2>
Let's load the census data.<br/>
Note that the last column from this dataset, <i>income</i>, will be our <b>target</b> label (whether an individual makes more than, or at most, $50,000 annually).<br/> 
All other columns are <b>features</b> about each individual in the census database.
<div class="linked"><script type="text/x-sage">
path='https://raw.githubusercontent.com/OlgaBelitskaya/machine_learning_engineer_nd009/master/Machine_Learning_Engineer_ND_P2/'
data=pandas.read_csv(path+'census.csv'); display(data.head().T); data.describe()
</script></div>
    <p></p>
    <h3>Implementation: Data Exploration</h3>
A cursory investigation of the dataset will determine how many individuals fit into either group, and will tell us about the percentage of these individuals making more than 50,000 USD.<br/> 
We need to compute the following:<br/>
- The total number of records, <i>n_records</i>.<br/>
- The number of individuals making more than 50,000 USD annually, <i>n_greater_50k</i>.<br/>
- The number of individuals making at most 50,000 USD annually, <i>n_at_most_50k</i>.<br/>
- The percentage of individuals making more than 50,000 USD annually, <i>greater_percent</i>.<br/>
<div class="linked"><script type="text/x-sage">
n_greater_50k=len(data[data['income']=='>50K']); n_at_most_50k=len(data[data['income']=='<=50K'])
n_records=len(data); greater_percent=float(n_greater_50k*100.0/n_records)
print ("Total number of records: {}".format(n_records))
print ("Individuals making more than $50,000: {}".format(n_greater_50k))
print ("Individuals making at most $50,000: {}".format(n_at_most_50k))
print ("Percentage of individuals making more than $50,000: {:.2f}%".format(greater_percent))
</script></div>
    <p></p>
    <h3>Features' Description</h3>
<b>age</b>: continuous.<br/>
<b>workclass</b>: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.<br/>
<b>education</b>: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.<br/>
<b>education-num</b>: continuous.<br/>
<b>marital-status</b>: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.<br/>
<b>occupation</b>: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, <br/>Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.<br/>
<b>relationship</b>: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.<br/>
<b>race</b>: Black, White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other.<br/>
<b>sex</b>: Female, Male.<br/>
<b>capital-gain</b>: continuous.<br/>
<b>capital-loss</b>: continuous.<br/>
<b>hours-per-week</b>: continuous.<br/>
<b>native-country</b>: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran,<br/>Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala,<br/>Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.
    <h2>Preparing the Data</h2>
Before data can be used as input for machine learning algorithms, it often must be cleaned, formatted, and restructured — this is typically known as <b>preprocessing</b>.<br/>
Fortunately, for this dataset, there are no invalid or missing entries we must deal with, however, there are some qualities about certain features that must be adjusted.<br/>
This preprocessing can help tremendously with the outcome and predictive power of nearly all learning algorithms.
    <h3>Transforming Skewed Continuous Features</h3>
A dataset may sometimes contain at least one feature whose values tend to lie near a single number,<br/>but will also have a non-trivial number of vastly larger or smaller values than that single number.<br/>
Algorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized.<br/>
With the census dataset two features fit this description: <i>capital-gain</i> and <i>capital-loss</i>.<br/>
Let's plot a histogram of these two features and have a look on the range of the values present and how they are distributed.
<div class="linked"><script type="text/x-sage">
income_raw=data['income']; features_raw=data.drop('income',axis=1); distribution(data)
</script></div>
    <p></p>
For highly-skewed feature distributions such as <i>capital-gain</i> and <i>capital-loss</i>, it is common practice to apply a <a href="https://en.wikipedia.org/wiki/Data_transformation_(statistics)">&#x1F578;logarithmic transformation</a> on the data <br/>
so that the very large and very small values do not negatively affect the performance of a learning algorithm.<br/>
Using a logarithmic transformation significantly reduces the range of values caused by outliers.<br/> 
Care must be taken when applying this transformation, however: the logarithm of 0 is undefined,<br/>
so we must translate the values by a small amount above 0 to apply the logarithm successfully.
<div class="linked"><script type="text/x-sage">
skewed=['capital-gain','capital-loss']; features_raw[skewed]=data[skewed].apply(lambda x:numpy.log(x+1))
distribution(features_raw,transformed=True)
</script></div>
    <p></p>
    <h3>Normalizing Numerical Features</h3>
In addition to performing transformations on features that are highly skewed, it is often good practice to perform some type of scaling on numerical features.<br/>
Applying a scaling to the data does not change the shape of each feature's distribution (such as <i>capital-gain</i> or <i>capital-loss</i> above);<br/>
however, normalization ensures that each feature is treated equally when applying supervised learners.<br/> 
Note that once scaling is applied, observing the data in its raw form will no longer have the same original meaning.     
<div class="linked"><script type="text/x-sage">
scaler=MinMaxScaler(); numerical=['age','education-num','capital-gain','capital-loss','hours-per-week']
features_raw[numerical]=scaler.fit_transform(data[numerical]); features_raw.head().T
</script></div>
    <p></p>
    <h3>Implementation: Data Preprocessing</h3>
From the table in <b>Exploring the Data</b> above, we can see there are several features for each record that are non-numeric.<br/> Typically, learning algorithms expect input to be numeric, which requires that non-numeric features (called <b>categorical variables</b>) be converted.<br/>
One popular way to convert categorical variables is by using the <b>one-hot encoding</b> scheme.<br/>
One-hot encoding creates a <b>"dummy"</b> variable for each possible category of each non-numeric feature.<br/>
For example, assume someFeature has three possible entries: A, B, or C.<br/>
We then encode this feature into <i>someFeature_A</i>, <i>someFeature_B</i> and <i>someFeature_C</i>.
<div class="linked"><script type="text/x-sage">
display(table([[' ','someFeature',' ','someFeature_A','someFeature_B','someFeature_C'],
               ['0','B',' ','0','1','0'],['1','C','=> one-hot encode =>','0','0','1'],['2','A',' ','1','0','0']]))
</script></div>
    <p></p>
Additionally, as with the non-numeric features, we need to convert the non-numeric target label, <i>income</i> to numerical values for the learning algorithm to work.<br/>
Since there are only two possible categories for this label ($'<=50K'$ and $'>50K'$),<br/>
we can avoid using one-hot encoding and simply encode these two categories as 0 and 1, respectively.<br/>
In code cell below, we will need to implement the following:<br/>
Use <i>pandas.get_dummies()</i> to perform one-hot encoding on the <i>features_raw</i> data.<br/>
Convert the target label <i>income_raw</i> to numerical entries.<br/>
Set records with $'<=50K'$ to 0 and records with $'>50K'$ to 1.
<div class="linked"><script type="text/x-sage">
categorical=['workclass','education_level','marital-status','occupation','relationship','race','sex','native-country']
features=pandas.DataFrame(features_raw); income=income_raw.replace(['<=50K','>50K'],[0,1]) 
for element in categorical: features[element]=pandas.get_dummies(features_raw[element])
encoded=list(features[categorical].columns) 
print ("{} total features after one-hot encoding.".format(len(encoded))); print (encoded)
</script></div>
    <p></p>
    <h3>Shuffle and Split Data</h3>
Now all <b>categorical</b> variables have been converted into <b>numerical</b> features, and all numerical features have been normalized.<br/> 
As always, we will now split the data (both features and their labels) into training and test sets.<br/> 
80% of the data will be used for training and 20% for testing.<br/>
<div class="linked"><script type="text/x-sage">
X_train,X_test,y_train,y_test=train_test_split(features,income,test_size=.2,random_state=0)
print ("Training set has {} samples.".format(X_train.shape[0])) 
print ("Testing set has {} samples.".format(X_test.shape[0]))
</script></div>
    <p></p>
    <h2>Evaluating Model Performance</h2>
In this section, we will investigate four different algorithms, and determine which is best at modeling the data.<br/>
Three of these algorithms will be supervised learners of your choice, and the fourth algorithm is known as a naive predictor.<br/>
    <h3>Metrics and the Naive Predictor</h3>
<b>CharityML</b>, equipped with their research, knows individuals that make more than 50,000 USD are most likely to donate to their charity.<br/> 
Because of this, CharityML is particularly interested in predicting who makes more than 50,000 USD accurately.<br/> 
It would seem that using <b>accuracy</b> as a metric for evaluating a particular model's performace would be appropriate.<br/>
Additionally, identifying someone that <b>does not make more than 50,000 USD</b> as someone who does would be detrimental to CharityML, since they are looking to find individuals willing to donate.<br/>
Therefore, a model's ability to <b>precisely</b> predict those that make more than 50,000 USD is <b>more important</b> than the model's ability to recall those individuals.<br/>
We can use <b>F-beta</b> score as a metric that considers both precision and recall:<br/>
<p>$F_{\beta} = (1 + \beta^2) \cdot = \frac {precision \cdot recall}{(\beta^2 \cdot precision) + recall}$</p>
In particular, when  $\beta = 0.5$ , more emphasis is placed on precision. This is called the $F_{0.5}$  score (or F-score for simplicity).<br/>
Looking at the distribution of classes (those who make at most 50,000 USD, and those who make more), it's clear most individuals do not make more than 50,000 USD.<br\>
This can greatly affect accuracy, since we could simply say "this person <b>does not make more than 50,000 USD</b>" and generally be right, without ever looking at the data!<br/> 
Making such a statement would be called naive, since we have not considered any information to substantiate the claim.<br/> 
It is always important to consider the <b>naive prediction</b> for your data, to help establish a benchmark for whether a model is performing well.<br/>
That been said, using that prediction would be pointless: If we predicted all people made less than 50,000 USD, CharityML would identify no one as donors.<br/>
<p>Note: Recap of accuracy, precision, recall.</p>
<b>Accuracy</b> measures how often the classifier makes the correct prediction.<br/>
It’s the ratio of the number of correct predictions to the total number of predictions (the number of test data points).
<p>$accuracy = \frac {number \ of \ correct \ predictions}{total \ number \ of \ predictions}$</p>
<b>Precision</b> tells us what proportion of data points we classified as individuals making more than 50,000 USD, actually made more than 50,000 USD.<br/>
It is a ratio of true positives to all positives (all points classified as individuals making more than 50,000 USD, irrespective of whether that was the correct classification),<br/>
in other words it is the ratio of
<p>$precision = \frac {true \ positives}{true \ positives + false \ positives}$</p>
<b>Recall</b> (sensitivity) tells us what proportion of individuals that actually made more than 50,000 USD were classified by us as individuals making more than 50,000 USD.<br/> 
It is a ratio of true positives to all individuals that actually made more than 50,000 USD, in other words it is the ratio of<br/>
<p>$recall = \frac {true \ positives}{true \ positives + false \ negatives}$</p>
For classification problems that are skewed in their classification distributions like in our case, accuracy by itself is not a very good metric.<br\>
Precision and recall help a lot and can be combined to get the F1 score, which is the weighted average (harmonic mean) of the precision and recall scores.<br/>
This score can range from 0 to 1, with 1 being the best possible F1 score (we take the harmonic mean as we are dealing with ratios).
    <h3>Question 1 - Naive Predictor Performace</h3>
<i>If we chose a model that always predicted an individual made more than 50,000 USD, what would that model's accuracy and F-score be on this dataset?</i>
    <h3>Answer 1</h3>
The code cell below displays both indicators in the output.
<div class="linked"><script type="text/x-sage">
beta=.5; accuracy=accuracy_score(income,numpy.array([1]*len(income)))
recall=1 # ==numpy.sum(income)/(numpy.sum(income)+0) 
precision=numpy.sum(income)/len(income)
fscore=(1 + beta**2)*(precision*recall)/((beta**2*precision)+recall)
fscore=fbeta_score(income,numpy.array([1]*len(income)),beta=0.5) # alternative method 
print ("Naive Predictor: accuracy score - {:.4f}, F-score - {:.4f}".format(accuracy,fscore))
</script></div>
    <p></p>
    <h3>Supervised Learning Models</h3>
The following supervised learning models are currently available in scikit-learn that you may choose from:<br/>
- Gaussian Naive Bayes (GaussianNB)<br/>
- Decision Trees<br/>
- Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting)<br/>
- K-Nearest Neighbors (KNeighbors)<br/>
- Stochastic Gradient Descent Classifier (SGDC)<br/>
- Support Vector Machines (SVM)<br/>
- Logistic Regression<br/>
    <h3>Question 2 - Model Application</h3>
<i>List three of the supervised learning models above that are appropriate for this problem that you will test on the census data.</i><br/><i>For each model chosen:</i><br/>
<i>- Describe one real-world application in industry where the model can be applied. (You may need to do research for this — give references!)</i><br/>
<i>- What are the strengths of the model; when does it perform well?</i><br/>
<i>- What are the weaknesses of the model; when does it perform poorly?</i><br/>
<i>- What makes this model a good candidate for the problem, given what you know about the data?</i><br/>
    <h3>Answer 2</h3>
I have chosen the following models: <i>GradientBoostingClassifier(); RandomForestClassifier(); AdaBoostClassifier()</i>.<br/> 
All of them are <b>ensemble</b> methods and combine the predictions of several base estimators to improve generalizability / robustness over a single estimator.<br/>
Let's have a look at their applications and characteristics:<br/>
1) <i>GradientBoostingClassifier</i>.<br/>
<b>Applications</b>: in the field of learning to rank (for example, web-seach), in ecology, etc.<br/>
<a href="http://proceedings.mlr.press/v14/mohan11a/mohan11a.pdf">&#x1F578;Web-Search Ranking with Initialized Gradient Boosted Regression Trees</a><br/>
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/">&#x1F578;Gradient boosting machines, a tutorial</a><br/>
The advantages and the disadvantages (Gradient Tree Boosting).<br/>
<b>Strengths</b>: natural handling of data of mixed type (= heterogeneous features), predictive power, robustness to outliers in output space (via robust loss functions).<br/>
<b>Weaknesses</b>: scalability, due to the sequential nature of boosting it can hardly be parallelized.<br/>
2) <i>RandomForestClassifier</i>.<br/>
<b>Applications</b>: in ecology, bioinformatics, etc.<br/>
<a href="https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/07-0539.1">&#x1F578;Random Forests for Classification in Ecology</a><br/>
<a href="http://www.cs.cmu.edu/~qyj/papersA08/11-rfbook.pdf">&#x1F578;Random Forest for Bioinformatics</a><br/>
The advantages and the disadvantages (Random Forests).<br/>
<b>Strengths</b>: runs efficiently on large data bases; gives estimates of what variables are important in the classification;<br/> maintains accuracy when a large proportion of the data are missing; high prediction accuracy.<br/>
<b>Weaknesses</b>: difficult to interpret, can be slow to evaluate.<br/>
3) <i>AdaBoostClassifier</i>.
<b>Applications</b>: the problem of face detection, text classification, etc.<br/>
<a href="">&#x1F578;AdaBoost-based face detection for embedded systems</a><br/>
<a href="">&#x1F578;Text Classification by Boosting Weak Learners based on Terms and Concepts</a><br/>
The advantages and the disadvantages (Ada Boost).<br/>
<b>Strengths</b>: can be used with data that is textual, numeric, discrete, etc.; can be combined with any other learning algorithm, not prone to overfitting; simple to implement.<br/>
<b>Weaknesses</b>: can be sensitive to noisy data and outliers; the performance depends on data and weak learner (can fail if weak classifiers too complex).<br/>
The outputs in our case are the variant of social ranking and it's a well-known fact that ensemble classifiers tend to be a better choice for this ranking.<br/>
All these algorithms will produce enough good predictions because of some reasons:<br/>
- they usually demonstrate high performance in practical tasks;<br/>
- do not so prone to overfitting;<br/>
- work well with mixed types of features (categorical and numeric).<br/>
      
<div class="linked"><script type="text/x-sage">
def train_predict(learner,sample_size,X_train,y_train,X_test,y_test):   
    results={}; start=time(); learner.fit(X_train[:sample_size],y_train[:sample_size]) 
    end=time(); results['train_time']=end-start
    start=time(); predictions_test=learner.predict(X_test); predictions_train=learner.predict(X_train[:300])
    end=time(); results['pred_time']=end-start
    results['acc_train']=accuracy_score(y_train[:300],predictions_train)
    results['acc_test']=accuracy_score(y_test,predictions_test)
    results['f_train']=fbeta_score(y_train[:300],predictions_train,average='macro',beta=0.5)
    results['f_test']=fbeta_score(y_test,predictions_test,average='macro',beta=0.5)
    print ("{} trained on {} samples.".format(learner.__class__.__name__,sample_size))
    return results
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
    <h3>Additional Code Cell</h3>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p> 
  </body>
</html>