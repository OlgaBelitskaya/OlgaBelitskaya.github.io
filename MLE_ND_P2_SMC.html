<!DOCTYPE HTML>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=0.9*device-width">
    <title>MLE_ND_P2_SMC</title>
    <script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script>
    <script>$(function () {
    sagecell.makeSagecell({inputLocation:'div.linked',linked:true,evalButtonText:'Run Linked Cells'});  
    sagecell.makeSagecell({inputLocation:'div.sage',evalButtonText:'Run'});
    });
    </script>
  </head>
  <style>
  @import url('https://fonts.googleapis.com/css?family=Orbitron|Roboto');
  body {background-color:mintcream;}; a {color:#00a050; font-family:'Roboto';} 
  h1 {color:#00A0A0; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;} 
  h2,h3 {color:slategray; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;}
  h4 {color:#00A0A0; font-family:'Roboto';}
  .sagecell .CodeMirror-scroll {min-height:3em; max-height:48em;}
  </style>  
  <body>
    <h1>Machine Learning Engineer Nanodegree</h1>
    <h2>Supervised Learning</h2>
    <h1>&#x1F4D1; &nbsp;P2: Finding Donors for CharityML</h1>
    <h2>Getting Started</h2>
    <h3>Data</h3>
In this project, we will employ several supervised algorithms of your choice to accurately model individuals' income using data collected from the 1994 U.S. Census.<br/>
We will then choose the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data.<br/>
The goal with this implementation is to construct a model that accurately predicts whether an individual makes more than $50,000.<br/>
This sort of task can arise in a non-profit setting, where organizations survive on donations.<br/>
Understanding an individual's income can help a non-profit better understand how large of a donation to request, or whether or not they should reach out to begin with.<br/>
While it can be difficult to determine an individual's general income bracket directly from public sources, we can (as we will see) infer this value from other publically available features.<br/>
The dataset for this project originates from the <a href="https://archive.ics.uci.edu/ml/datasets/Census+Income">&#x1F578;UCI Machine Learning Repository.</a><br/>
The datset was donated by Ron Kohavi and Barry Becker, after being published in the article "Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid".You can find the article by Ron Kohavi <a href="https://www.aaai.org/Papers/KDD/1996/KDD96-033.pdf">&#x1F578;online.</a><br/>
The data we investigate here consists of small changes to the original dataset, such as removing the <i>fnlwgt</i> feature and records with missing or ill-formatted entries.
      <h3>Resources</h3>
<a href="http://archive.ics.uci.edu/ml/datasets.php">&#x1F578;UCI Machine Learning Repository&nbsp;</a>
<a href="https://scikit-learn.org/stable/index.html">&#x1F578;scikit-learn. Machine Learning in Python&nbsp;</a>
<a href="http://seaborn.pydata.org/index.html">&#x1F578;seaborn: statistical data visualization&nbsp;</a>
      <h3>Code Library</h3> 
<div class="linked"><script type="text/x-sage">
import numpy,pandas,pylab,seaborn,time; pylab.style.use('seaborn-whitegrid')
import warnings; warnings.filterwarnings("ignore",category=FutureWarning)
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.metrics import f1_score,accuracy_score,fbeta_score,confusion_matrix,make_scorer
from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier,RandomForestClassifier
from sklearn.svm import LinearSVC; from sklearn.linear_model import SGDClassifier
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
# https://github.com/udacity/machine-learning/blob/master/projects/finding_donors/visuals.py
def distribution(data,transformed=False):
    fig=ylab.figure(figsize=(12,5));
    for i,feature in enumerate(['capital-gain','capital-loss']):
        ax=fig.add_subplot(1,2,i+1); ax.hist(data[feature],bins=25,color='#00A0A0')
        ax.set_title("'%s' Feature Distribution"%(feature),fontsize=14)
        ax.set_xlabel("Value"); ax.set_ylabel("Number of Records"); ax.set_ylim((0,2000)) 
        ax.set_yticks([0,500,1000,1500,2000]); ax.set_yticklabels([0,500,1000,1500,">2000"])
    if transformed:
        fig.suptitle("Log-transformed Distributions of Continuous Census Data Features",fontsize=16,y=1.03)
    else:
        fig.suptitle("Skewed Distributions of Continuous Census Data Features",fontsize=16,y=1.03)
    fig.tight_layout(); pylab.show()
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
def evaluate(results,accuracy,f1):
    fig,ax=pylab.subplots(2,3,figsize=(12,5))
    bar_width=0.3; colors=['#A00000','#00A0A0','#00A000']
    for k,learner in enumerate(results.keys()):
        for j,metric in enumerate(['train_time','acc_train','f_train','pred_time','acc_test','f_test']):
            for i in np.arange(3):
                ax[int(j/3),j%3].bar(i+k*bar_width,results[learner][i][metric],width=bar_width,color=colors[k])
                ax[int(j/3),j%3].set_xticks([0.45,1.45,2.45]); ax[int(j/3),j%3].set_xticklabels(["1%","10%","100%"])
                ax[int(j/3),j%3].set_xlabel("Training Set Size"); ax[int(j/3),j%3].set_xlim((-0.1,3.0))
    ax[0,0].set_ylabel("Time (in seconds)"); ax[0,1].set_ylabel("Accuracy Score"); ax[0,2].set_ylabel("F-score")
    ax[1,0].set_ylabel("Time (in seconds)"); ax[1,1].set_ylabel("Accuracy Score"); ax[1,2].set_ylabel("F-score")
    ax[0,0].set_title("Model Training"); ax[0,1].set_title("Accuracy Score on Training Subset")
    ax[0,2].set_title("F-score on Training Subset"); ax[1,0].set_title("Model Predicting")
    ax[1,1].set_title("Accuracy Score on Testing Set"); ax[1,2].set_title("F-score on Testing Set")
    ax[0,1].axhline(y=accuracy,xmin=-0.1,xmax=3.0,linewidth=1,color='k',linestyle='dashed')
    ax[1,1].axhline(y=accuracy,xmin=-0.1,xmax=3.0,linewidth=1,color='k',linestyle='dashed')
    ax[0,2].axhline(y=f1,xmin=-0.1,xmax=3.0,linewidth=1,color='k',linestyle='dashed')
    ax[1,2].axhline(y=f1,xmin=-0.1,xmax=3.0,linewidth=1,color='k',linestyle='dashed')
    ax[0,1].set_ylim((0,1)); ax[0,2].set_ylim((0,1)); ax[1,1].set_ylim((0,1)); ax[1,2].set_ylim((0,1))
    patches = []
    for i,learner in enumerate(results.keys()): patches.append(mpatches.Patch(color=colors[i],label=learner))
    pylab.legend(handles=patches,bbox_to_anchor=(-0.8,2.53),loc='upper center',borderaxespad=0.,ncol=3,fontsize='x-large')
    pylab.suptitle("Performance Metrics for Three Supervised Learning Models",fontsize=16,y=1.15)
    pylab.tight_layout(); pylab.show()
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
def feature_plot(importances,X_train,y_train):
    indices=numpy.argsort(importances)[::-1]; columns=X_train.columns.values[indices[:5]]
    values=importances[indices][:5]; fig=pylab.figure(figsize=(12,5))
    pylab.title("Normalized Weights for First Five Most Predictive Features",fontsize=16)
    pylab.bar(numpy.arange(5),values,width=0.6,align="center",color='#00A000',label="Feature Weight")
    pylab.bar(numpy.arange(5)-0.3,numpy.cumsum(values),width=0.2,align="center",color='#00A0A0',label="Cumulative Feature Weight")
    pylab.xticks(numpy.arange(5),columns); pylab.xlim((-0.5,4.5))
    pylab.ylabel("Weight",fontsize=12); pylab.xlabel("Feature",fontsize=12)   
    pylab.legend(loc='upper center'); pylab.tight_layout(); pylab.show()  
</script></div>
    <p></p>
    <h2>Exploring the Data</h2>
Let's load the census data.<br/>
Note that the last column from this dataset, <i>income</i>, will be our <b>target</b> label (whether an individual makes more than, or at most, $50,000 annually).<br/> 
All other columns are <b>features</b> about each individual in the census database.
<div class="linked"><script type="text/x-sage">
path='https://raw.githubusercontent.com/OlgaBelitskaya/machine_learning_engineer_nd009/master/Machine_Learning_Engineer_ND_P2/'
data=pandas.read_csv(path+'census.csv'); display(data.head().T); data.describe()
</script></div>
    <p></p>
    <h3>Implementation: Data Exploration</h3>
A cursory investigation of the dataset will determine how many individuals fit into either group, and will tell us about the percentage of these individuals making more than 50,000 USD.<br/> 
We need to compute the following:<br/>
- The total number of records, <i>n_records</i>.<br/>
- The number of individuals making more than 50,000 USD annually, <i>n_greater_50k</i>.<br/>
- The number of individuals making at most 50,000 USD annually, <i>n_at_most_50k</i>.<br/>
- The percentage of individuals making more than 50,000 USD annually, <i>greater_percent</i>.<br/>
<div class="linked"><script type="text/x-sage">
n_greater_50k=len(data[data['income']=='>50K']); n_at_most_50k=len(data[data['income']=='<=50K'])
n_records=len(data); greater_percent=float(n_greater_50k*100.0/n_records)
print ("Total number of records: {}".format(n_records))
print ("Individuals making more than $50,000: {}".format(n_greater_50k))
print ("Individuals making at most $50,000: {}".format(n_at_most_50k))
print ("Percentage of individuals making more than $50,000: {:.2f}%".format(greater_percent))
</script></div>
    <p></p>
    <h3>Features' Description</h3>
<b>age</b>: continuous.<br/>
<b>workclass</b>: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.<br/>
<b>education</b>: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.<br/>
<b>education-num</b>: continuous.<br/>
<b>marital-status</b>: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.<br/>
<b>occupation</b>: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, <br/>Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.<br/>
<b>relationship</b>: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.<br/>
<b>race</b>: Black, White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other.<br/>
<b>sex</b>: Female, Male.<br/>
<b>capital-gain</b>: continuous.<br/>
<b>capital-loss</b>: continuous.<br/>
<b>hours-per-week</b>: continuous.<br/>
<b>native-country</b>: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran,<br/>Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala,<br/>Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.
    <h2>Preparing the Data</h2>
Before data can be used as input for machine learning algorithms, it often must be cleaned, formatted, and restructured — this is typically known as <b>preprocessing</b>.<br/>
Fortunately, for this dataset, there are no invalid or missing entries we must deal with, however, there are some qualities about certain features that must be adjusted.<br/>
This preprocessing can help tremendously with the outcome and predictive power of nearly all learning algorithms.
    <h3>Transforming Skewed Continuous Features</h3>
A dataset may sometimes contain at least one feature whose values tend to lie near a single number,<br/>but will also have a non-trivial number of vastly larger or smaller values than that single number.<br/>
Algorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized.<br/>
With the census dataset two features fit this description: <i>capital-gain</i> and <i>capital-loss</i>.<br/>
Let's plot a histogram of these two features and have a look on the range of the values present and how they are distributed.
<div class="linked"><script type="text/x-sage">
income_raw=data['income']; features_raw=data.drop('income',axis=1); distribution(data)
</script></div>
    <p></p>
For highly-skewed feature distributions such as <i>capital-gain</i> and <i>capital-loss</i>, it is common practice to apply a <a href="https://en.wikipedia.org/wiki/Data_transformation_(statistics)">&#x1F578;logarithmic transformation</a> on the data <br/>
so that the very large and very small values do not negatively affect the performance of a learning algorithm.<br/>
Using a logarithmic transformation significantly reduces the range of values caused by outliers.<br/> 
Care must be taken when applying this transformation, however: the logarithm of 0 is undefined, so we must translate the values by a small amount above 0 to apply the logarithm successfully.
<div class="linked"><script type="text/x-sage">
skewed=['capital-gain','capital-loss']; features_raw[skewed]=data[skewed].apply(lambda x:numpy.log(x+1))
distribution(features_raw,transformed=True)
</script></div>
    <p></p>
      
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
    <h3>Additional Code Cell</h3>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p> 
  </body>
</html>