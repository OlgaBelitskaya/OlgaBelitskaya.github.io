<!DOCTYPE HTML>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=0.9*device-width">
    <title>MLE_ND_P2_SMC</title>
    <script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script>
    <script>$(function () {
    sagecell.makeSagecell({inputLocation:'div.linked',linked:true,evalButtonText:'Run Linked Cells'});  
    sagecell.makeSagecell({inputLocation:'div.sage',evalButtonText:'Run'});
    });
    </script>
  </head>
  <style>
  @import url('https://fonts.googleapis.com/css?family=Orbitron|Roboto');
  body {background-color:mintcream;}; a {color:#00a050; font-family:'Roboto';} 
  h1 {color:#00A0A0; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;} 
  h2,h3 {color:slategray; font-family:'Orbitron'; text-shadow:4px 4px 4px #ccc;}
  h4 {color:#00A0A0; font-family:'Roboto';}
  .sagecell .CodeMirror-scroll {min-height:3em; max-height:48em;}
  </style>  
  <body>
    <h1>Machine Learning Engineer Nanodegree</h1>
    <h2>Supervised Learning</h2>
    <h1>&#x1F4D1; &nbsp;P2: Finding Donors for CharityML</h1>
    <h2>Getting Started</h2>
    <h3>Data</h3>
In this project, we will employ several supervised algorithms of your choice to accurately model individuals' income using data collected from the 1994 U.S. Census.<br/>
We will then choose the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data.<br/>
The goal with this implementation is to construct a model that accurately predicts whether an individual makes more than $50,000.<br/>
This sort of task can arise in a non-profit setting, where organizations survive on donations.<br/>
Understanding an individual's income can help a non-profit better understand how large of a donation to request, or whether or not they should reach out to begin with.<br/>
While it can be difficult to determine an individual's general income bracket directly from public sources, we can (as we will see) infer this value from other publically available features.<br/>
The dataset for this project originates from the <a href="https://archive.ics.uci.edu/ml/datasets/Census+Income">&#x1F578;UCI Machine Learning Repository.</a><br/>
The datset was donated by Ron Kohavi and Barry Becker, after being published in the article "Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid".You can find the article by Ron Kohavi <a href="https://www.aaai.org/Papers/KDD/1996/KDD96-033.pdf">&#x1F578;online.</a><br/>
The data we investigate here consists of small changes to the original dataset, such as removing the <i>fnlwgt</i> feature and records with missing or ill-formatted entries.
      <h3>Resources</h3>
<a href="http://archive.ics.uci.edu/ml/datasets.php">&#x1F578;UCI Machine Learning Repository&nbsp;</a>
<a href="https://scikit-learn.org/stable/index.html">&#x1F578;scikit-learn. Machine Learning in Python&nbsp;</a>
<a href="http://seaborn.pydata.org/index.html">&#x1F578;seaborn: statistical data visualization&nbsp;</a>
      <h3>Code Library</h3> 
<div class="linked"><script type="text/x-sage">
import numpy,pandas,pylab,seaborn,time; pylab.style.use('seaborn-whitegrid')
from sklearn.exceptions import DataConversionWarning
import warnings; warnings.filterwarnings("ignore",category=[FutureWarning,DataConversionWarning])
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.metrics import f1_score,accuracy_score,fbeta_score,confusion_matrix,make_scorer
from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier,RandomForestClassifier
from sklearn.svm import LinearSVC; from sklearn.linear_model import SGDClassifier
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
# https://github.com/udacity/machine-learning/blob/master/projects/finding_donors/visuals.py
def distribution(data,transformed=False):
    fig=pylab.figure(figsize=(12,5));
    for i,feature in enumerate(['capital-gain','capital-loss']):
        ax=fig.add_subplot(1,2,i+1); ax.hist(data[feature],bins=30,color='#00A0A0')
        ax.set_title("'%s' Feature Distribution"%(feature),fontsize=14)
        ax.set_xlabel("Value"); ax.set_ylabel("Number of Records"); ax.set_ylim((0,2000)) 
        ax.set_yticks([0,500,1000,1500,2000]); ax.set_yticklabels([0,500,1000,1500,">2000"])
    if transformed:
        fig.suptitle("Log-transformed Distributions of Continuous Census Data Features",fontsize=16,y=1.03)
    else:
        fig.suptitle("Skewed Distributions of Continuous Census Data Features",fontsize=16,y=1.03)
    fig.tight_layout(); pylab.show()
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
def evaluate(results,accuracy,f1):
    fig,ax=pylab.subplots(2,3,figsize=(12,5))
    bar_width=0.3; colors=['#A00000','#00A0A0','#00A000']
    for k,learner in enumerate(results.keys()):
        for j,metric in enumerate(['train_time','acc_train','f_train','pred_time','acc_test','f_test']):
            for i in np.arange(3):
                ax[int(j/3),j%3].bar(i+k*bar_width,results[learner][i][metric],width=bar_width,color=colors[k])
                ax[int(j/3),j%3].set_xticks([0.45,1.45,2.45]); ax[int(j/3),j%3].set_xticklabels(["1%","10%","100%"])
                ax[int(j/3),j%3].set_xlabel("Training Set Size"); ax[int(j/3),j%3].set_xlim((-0.1,3.0))
    ax[0,0].set_ylabel("Time (in seconds)"); ax[0,1].set_ylabel("Accuracy Score"); ax[0,2].set_ylabel("F-score")
    ax[1,0].set_ylabel("Time (in seconds)"); ax[1,1].set_ylabel("Accuracy Score"); ax[1,2].set_ylabel("F-score")
    ax[0,0].set_title("Model Training"); ax[0,1].set_title("Accuracy Score on Training Subset")
    ax[0,2].set_title("F-score on Training Subset"); ax[1,0].set_title("Model Predicting")
    ax[1,1].set_title("Accuracy Score on Testing Set"); ax[1,2].set_title("F-score on Testing Set")
    ax[0,1].axhline(y=accuracy,xmin=-0.1,xmax=3.0,linewidth=1,color='k',linestyle='dashed')
    ax[1,1].axhline(y=accuracy,xmin=-0.1,xmax=3.0,linewidth=1,color='k',linestyle='dashed')
    ax[0,2].axhline(y=f1,xmin=-0.1,xmax=3.0,linewidth=1,color='k',linestyle='dashed')
    ax[1,2].axhline(y=f1,xmin=-0.1,xmax=3.0,linewidth=1,color='k',linestyle='dashed')
    ax[0,1].set_ylim((0,1)); ax[0,2].set_ylim((0,1)); ax[1,1].set_ylim((0,1)); ax[1,2].set_ylim((0,1))
    patches = []
    for i,learner in enumerate(results.keys()): patches.append(mpatches.Patch(color=colors[i],label=learner))
    pylab.legend(handles=patches,bbox_to_anchor=(-0.8,2.53),loc='upper center',borderaxespad=0.,ncol=3,fontsize='x-large')
    pylab.suptitle("Performance Metrics for Three Supervised Learning Models",fontsize=16,y=1.15)
    pylab.tight_layout(); pylab.show()
</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">
def feature_plot(importances,X_train,y_train):
    indices=numpy.argsort(importances)[::-1]; columns=X_train.columns.values[indices[:5]]
    values=importances[indices][:5]; fig=pylab.figure(figsize=(12,5))
    pylab.title("Normalized Weights for First Five Most Predictive Features",fontsize=16)
    pylab.bar(numpy.arange(5),values,width=0.6,align="center",color='#00A000',label="Feature Weight")
    pylab.bar(numpy.arange(5)-0.3,numpy.cumsum(values),width=0.2,align="center",color='#00A0A0',label="Cumulative Feature Weight")
    pylab.xticks(numpy.arange(5),columns); pylab.xlim((-0.5,4.5))
    pylab.ylabel("Weight",fontsize=12); pylab.xlabel("Feature",fontsize=12)   
    pylab.legend(loc='upper center'); pylab.tight_layout(); pylab.show()  
</script></div>
    <p></p>
    <h2>Exploring the Data</h2>
Let's load the census data.<br/>
Note that the last column from this dataset, <i>income</i>, will be our <b>target</b> label (whether an individual makes more than, or at most, $50,000 annually).<br/> 
All other columns are <b>features</b> about each individual in the census database.
<div class="linked"><script type="text/x-sage">
path='https://raw.githubusercontent.com/OlgaBelitskaya/machine_learning_engineer_nd009/master/Machine_Learning_Engineer_ND_P2/'
data=pandas.read_csv(path+'census.csv'); display(data.head().T); data.describe()
</script></div>
    <p></p>
    <h3>Implementation: Data Exploration</h3>
A cursory investigation of the dataset will determine how many individuals fit into either group, and will tell us about the percentage of these individuals making more than 50,000 USD.<br/> 
We need to compute the following:<br/>
- The total number of records, <i>n_records</i>.<br/>
- The number of individuals making more than 50,000 USD annually, <i>n_greater_50k</i>.<br/>
- The number of individuals making at most 50,000 USD annually, <i>n_at_most_50k</i>.<br/>
- The percentage of individuals making more than 50,000 USD annually, <i>greater_percent</i>.<br/>
<div class="linked"><script type="text/x-sage">
n_greater_50k=len(data[data['income']=='>50K']); n_at_most_50k=len(data[data['income']=='<=50K'])
n_records=len(data); greater_percent=float(n_greater_50k*100.0/n_records)
print ("Total number of records: {}".format(n_records))
print ("Individuals making more than $50,000: {}".format(n_greater_50k))
print ("Individuals making at most $50,000: {}".format(n_at_most_50k))
print ("Percentage of individuals making more than $50,000: {:.2f}%".format(greater_percent))
</script></div>
    <p></p>
    <h3>Features' Description</h3>
<b>age</b>: continuous.<br/>
<b>workclass</b>: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.<br/>
<b>education</b>: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.<br/>
<b>education-num</b>: continuous.<br/>
<b>marital-status</b>: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.<br/>
<b>occupation</b>: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, <br/>Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.<br/>
<b>relationship</b>: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.<br/>
<b>race</b>: Black, White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other.<br/>
<b>sex</b>: Female, Male.<br/>
<b>capital-gain</b>: continuous.<br/>
<b>capital-loss</b>: continuous.<br/>
<b>hours-per-week</b>: continuous.<br/>
<b>native-country</b>: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran,<br/>Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala,<br/>Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.
    <h2>Preparing the Data</h2>
Before data can be used as input for machine learning algorithms, it often must be cleaned, formatted, and restructured â€” this is typically known as <b>preprocessing</b>.<br/>
Fortunately, for this dataset, there are no invalid or missing entries we must deal with, however, there are some qualities about certain features that must be adjusted.<br/>
This preprocessing can help tremendously with the outcome and predictive power of nearly all learning algorithms.
    <h3>Transforming Skewed Continuous Features</h3>
A dataset may sometimes contain at least one feature whose values tend to lie near a single number,<br/>but will also have a non-trivial number of vastly larger or smaller values than that single number.<br/>
Algorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized.<br/>
With the census dataset two features fit this description: <i>capital-gain</i> and <i>capital-loss</i>.<br/>
Let's plot a histogram of these two features and have a look on the range of the values present and how they are distributed.
<div class="linked"><script type="text/x-sage">
income_raw=data['income']; features_raw=data.drop('income',axis=1); distribution(data)
</script></div>
    <p></p>
For highly-skewed feature distributions such as <i>capital-gain</i> and <i>capital-loss</i>, it is common practice to apply a <a href="https://en.wikipedia.org/wiki/Data_transformation_(statistics)">&#x1F578;logarithmic transformation</a> on the data <br/>
so that the very large and very small values do not negatively affect the performance of a learning algorithm.<br/>
Using a logarithmic transformation significantly reduces the range of values caused by outliers.<br/> 
Care must be taken when applying this transformation, however: the logarithm of 0 is undefined,<br/>
so we must translate the values by a small amount above 0 to apply the logarithm successfully.
<div class="linked"><script type="text/x-sage">
skewed=['capital-gain','capital-loss']; features_raw[skewed]=data[skewed].apply(lambda x:numpy.log(x+1))
distribution(features_raw,transformed=True)
</script></div>
    <p></p>
    <h3>Normalizing Numerical Features</h3>
In addition to performing transformations on features that are highly skewed, it is often good practice to perform some type of scaling on numerical features.<br/>
Applying a scaling to the data does not change the shape of each feature's distribution (such as <i>capital-gain</i> or <i>capital-loss</i> above);<br/>
however, normalization ensures that each feature is treated equally when applying supervised learners.<br/> 
Note that once scaling is applied, observing the data in its raw form will no longer have the same original meaning.     
<div class="linked"><script type="text/x-sage">
scaler=MinMaxScaler(); numerical=['age','education-num','capital-gain','capital-loss','hours-per-week']
features_raw[numerical]=scaler.fit_transform(data[numerical]); features_raw.head().T
</script></div>
    <p></p>
    <h3>Implementation: Data Preprocessing</h3>
From the table in <b>Exploring the Data</b> above, we can see there are several features for each record that are non-numeric.<br/> Typically, learning algorithms expect input to be numeric, which requires that non-numeric features (called <b>categorical variables</b>) be converted.<br/>
One popular way to convert categorical variables is by using the <b>one-hot encoding</b> scheme.<br/>
One-hot encoding creates a <b>"dummy"</b> variable for each possible category of each non-numeric feature.<br/>
For example, assume someFeature has three possible entries: A, B, or C.<br/>
We then encode this feature into <i>someFeature_A</i>, <i>someFeature_B</i> and <i>someFeature_C</i>.
<div class="linked"><script type="text/x-sage">
display(table([[' ','someFeature',' ','someFeature_A','someFeature_B','someFeature_C'],
               ['0','B',' ','0','1','0'],['1','C','=> one-hot encode =>','0','0','1'],['2','A',' ','1','0','0']]))
</script></div>
    <p></p>
Additionally, as with the non-numeric features, we need to convert the non-numeric target label, <i>income</i> to numerical values for the learning algorithm to work.<br/>
Since there are only two possible categories for this label ($'<=50K'$ and $'>50K'$),<br/>
we can avoid using one-hot encoding and simply encode these two categories as 0 and 1, respectively.<br/>
In code cell below, we will need to implement the following:<br/>
Use <i>pandas.get_dummies()</i> to perform one-hot encoding on the <i>features_raw</i> data.<br/>
Convert the target label <i>income_raw</i> to numerical entries.<br/>
Set records with $'<=50K'$ to 0 and records with $'>50K'$ to 1.
<div class="linked"><script type="text/x-sage">
categorical=['workclass','education_level','marital-status','occupation','relationship','race','sex','native-country']
features=pandas.DataFrame(features_raw); income=income_raw.replace(['<=50K','>50K'],[0,1]) 
for element in categorical: features[element]=pandas.get_dummies(features_raw[element])
encoded=list(features[categorical].columns) 
print ("{} total features after one-hot encoding.".format(len(encoded))); print (encoded)
</script></div>
    <p></p>
    <h3>Shuffle and Split Data</h3>
Now all <b>categorical</b> variables have been converted into <b>numerical</b> features, and all numerical features have been normalized.<br/> 
As always, we will now split the data (both features and their labels) into training and test sets.<br/> 
80% of the data will be used for training and 20% for testing.<br/>
<div class="linked"><script type="text/x-sage">
X_train,X_test,y_train,y_test=train_test_split(features,income,test_size=.2,random_state=0)
print ("Training set has {} samples.".format(X_train.shape[0])) 
print ("Testing set has {} samples.".format(X_test.shape[0]))
</script></div>
    <p></p>
    <h2>Evaluating Model Performance</h2>
In this section, we will investigate four different algorithms, and determine which is best at modeling the data.<br/>
Three of these algorithms will be supervised learners of your choice, and the fourth algorithm is known as a naive predictor.<br/>
      <h3>Metrics and the Naive Predictor</h3>
<b>CharityML</b>, equipped with their research, knows individuals that make more than 50,000 USD are most likely to donate to their charity.<br/> 
Because of this, CharityML is particularly interested in predicting who makes more than 50,000 USD accurately.<br/> 
It would seem that using <b>accuracy</b> as a metric for evaluating a particular model's performace would be appropriate.<br/>
Additionally, identifying someone that <b>does not make more than 50,000 USD</b> as someone who does would be detrimental to CharityML, since they are looking to find individuals willing to donate.<br/>
Therefore, a model's ability to <b>precisely</b> predict those that make more than 50,000 USD is <b>more important</b> than the model's ability to recall those individuals.<br/>
We can use <b>F-beta</b> score as a metric that considers both precision and recall:<br/>
<p>$F_{\beta} = (1 + \beta^2) \cdot$</p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p>
    <h3>Additional Code Cell</h3>
<div class="linked"><script type="text/x-sage">

</script></div>
    <p></p> 
  </body>
</html>