<!DOCTYPE HTML>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <title>üèôDLPP2SMC</title>
    <script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script>
    <script>$(function (){
    sagecell.makeSagecell({inputLocation:'div.linked',linked:true,
                           evalButtonText:'Run Linked Cells'}); });
    </script>
  </head>
  <style>
  @import url('https://fonts.googleapis.com/css?family=Akronim|Lobster');
  h1,h2,th {color:#348ABD; font-family:'Akronim'; font-size:120%; text-shadow:4px 4px 4px #aaa;}
  p,a {color:slategray; font-family:'Lobster'; font-size:120%; text-shadow:4px 4px 4px #aaa;}
  .sagecell .CodeMirror-scroll {min-height:3em; max-height:30em;}
  body {margin:5px 5px 5px 15px;}
  </style>  
  <body>
    <h1>üìë &nbsp; Deep Learning. P2: Multi-Label Classification. Letter Recognition</h1>
<a href="https://olgabelitskaya.github.io/README.html">&#x1F300; &nbsp; Home Page &nbsp; &nbsp; &nbsp;</a>    
<a href="https://www.instagram.com/olga.belitskaya/">&#x1F300; &nbsp; Instagram Posts &nbsp; &nbsp; &nbsp;</a>     
<a href="https://www.pinterest.ru/olga_belitskaya/code-style/">&#x1F300; &nbsp; Pinterest Posts</a><br/>
For this project, I have created the dataset of <br/>
14190 color images (32x32x3) with 33 handwritten Russian letters.<br/>
There are four types of letter backgrounds here. <br/>
They are labeled in this dataset as well.<br/>
<div class="linked"><script type="text/x-sage">
%%html
<style>
@import url('https://fonts.googleapis.com/css?family=Akronim|Ruthie');
</style>
<table style="width:80%; background-color:black; 
              font-family:Ruthie; font-size:200%;">

</table>
</script></div><br/>
    <h2>‚úíÔ∏è&nbsp;Step 0. Importing Libraries and Defining Helpful Functions</h2>      
<div class="linked"><script type="text/x-sage">
#unlock to install modules
#!python3 -m pip install keras --user
#!python3 -m pip install --ignore-installed --upgrade tensorflow==1.14.0
</script></div><br/>
<div class="linked"><script type="text/x-sage">
spath='/home/sc_work/.sage/local/lib/python3.7/site-packages'
import sys; sys.path.append(spath)
import h5py,urllib,zipfile
import pandas as pd,numpy as np,pylab as pl
import keras as ks,tensorflow as tf
import warnings; warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from keras.callbacks import ModelCheckpoint,EarlyStopping
from keras.callbacks import ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential,load_model
from keras.layers import Activation,Dense,LSTM
from keras.layers import Flatten,Dropout,BatchNormalization
from keras.layers import Conv2D,MaxPooling2D,GlobalMaxPooling2D
from keras.layers.advanced_activations import PReLU,LeakyReLU
i0,i1,i2=int(0),int(1),int(2)
np.set_printoptions(precision=6)
from keras import __version__
print('keras version:', __version__)
print('tensorflow version:',tf.__version__)
</script></div><br/>
<div class="linked"><script type="text/x-sage">
def ohe(x): 
    return OneHotEncoder(categories='auto')\
           .fit(x.reshape(-i1,i1))\
           .transform(x.reshape(-i1,i1))\
           .toarray().astype('int64')
def tts(X,y): 
    x_train,x_test,y_train,y_test=\
    train_test_split(X,y,test_size=float(.2),
                     random_state=i1)
    n=int(len(x_test)/2)
    x_valid,y_valid=x_test[:n],y_test[:n]
    x_test,y_test=x_test[n:],y_test[n:]
    return x_train,x_valid,x_test,y_train,y_valid,y_test
</script></div><br/> 
<div class="linked"><script type="text/x-sage">
def history_plot(fit_history):
    pl.figure(figsize=(12,10)); pl.subplot(211)
    pl.plot(fit_history.history['loss'],
            color='slategray',label='train')
    pl.plot(fit_history.history['val_loss'],
            color='#348ABD',label='valid')
    pl.xlabel("Epochs"); pl.ylabel("Loss")
    pl.legend();pl.title('Loss Function')     
    pl.subplot(212)
    pl.plot(fit_history.history['acc'],
            color='slategray',label='train')
    pl.plot(fit_history.history['val_acc'],
            color='#348ABD',label='valid')
    pl.xlabel("Epochs"); pl.ylabel("Accuracy")    
    pl.legend(); pl.title('Accuracy')
    pl.show()
</script></div><br/> 
    <h2>‚úíÔ∏è&nbsp;Step 1. Loading and Preprocessing the Data</h2>       
<div class="linked"><script type="text/x-sage">
fpath='https://olgabelitskaya.github.io/'
zf='LetterColorImages_123.h5.zip'
input_file=urllib.request.urlopen(fpath+zf)
output_file=open(zf,'wb')
output_file.write(input_file.read())
output_file.close(); input_file.close()
zipf=zipfile.ZipFile(zf,'r')
zipf.extractall(''); zipf.close()
f=h5py.File(zf[:-4],'r')
keys=list(f.keys())
letters=u'–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è'
backgrounds=np.array(f[keys[0]])
images=np.array(f[keys[1]])/255
labels=np.array(f[keys[2]])
pl.figure(figsize=(2,3)); il=10**4
pl.xticks([]); pl.yticks([])
pl.title('Label: %s \n'%letters[labels[il]-1]+\
         'Background: %s'%backgrounds[il],
         fontsize=18)
pl.imshow(images[il]); pl.show()
</script></div><br/>
<div class="linked"><script type="text/x-sage">
gray_images=np.dot(images[...,:3],[.299,.587,.114])
pl.figure(figsize=(2,3))
pl.title('Label: %s \n'%letters[labels[il]-1]+\
         'Background: %s'%backgrounds[il],
         fontsize=18)
pl.imshow(gray_images[il],cmap=pl.cm.bone)
pl.xticks([]); pl.yticks([]); pl.show()
</script></div><br/>
<div class="linked"><script type="text/x-sage">
cbackgrounds,clabels=ohe(backgrounds),ohe(labels)
pd.DataFrame([labels[97:103],clabels[97:103]]).T
</script></div><br/>
<div class="linked"><script type="text/x-sage">
ctargets=np.concatenate((clabels,cbackgrounds),axis=1)
pd.DataFrame([clabels.shape,cbackgrounds.shape,ctargets.shape])
</script></div><br/>
<div class="linked"><script type="text/x-sage">
x_train1,x_valid1,x_test1,\
y_train1,y_valid1,y_test1=tts(images,clabels)
x_train2,x_valid2,x_test2,\
y_train2,y_valid2,y_test2=tts(gray_images,clabels)
x_train3,x_valid3,x_test3,\
y_train3,y_valid3,y_test3=tts(images,ctargets)
x_train4,x_valid4,x_test4,\
y_train4,y_valid4,y_test4=tts(gray_images,ctargets)
sh=[el.shape for el in \
[x_train1,y_train1,x_valid1,y_valid1,x_test1,y_test1,
 x_train2,y_train2,x_valid2,y_valid2,x_test2,y_test2,
 x_train3,y_train3,x_valid3,y_valid3,x_test3,y_test3,
 x_train4,y_train4,x_valid4,y_valid4,x_test4,y_test4]]
pd.DataFrame(sh)
</script></div><br/>
<div class="linked"><script type="text/x-sage">
y_train3_list=[y_train3[:,:33],y_train3[:,33:]]
y_valid3_list=[y_valid3[:,:33],y_valid3[:,33:]]
y_test3_list=[y_test3[:,:33],y_test3[:,33:]]
y_train4_list=[y_train4[:,:33],y_train4[:,33:]]
y_valid4_list=[y_valid4[:,:33],y_valid4[:,33:]]
y_test4_list=[y_test4[:,:33],y_test4[:,33:]]
</script></div><br/>
      <h2>‚úíÔ∏è &nbsp; Step 2. One-Label Classification Models</h2>
<p>Color Images</p>
<div class="linked"><script type="text/x-sage">
fw='weights.letter.hdf5'
dr,fr,al=float(.2),float(.2),float(.02)
i0,i1,i2,i5=int(0),int(1),int(2),int(5)
i32,i33,i64,i96=int(32),int(33),int(64),int(96)
i100,i128,i196,i200=int(100),int(128),int(196),int(200)
i512,i1024=int(512),int(1024)
n=int(7000)
</script></div><br/>
<div class="linked"><script type="text/x-sage">
def model():
    model=Sequential()
    model.add(Conv2D(i32,(i5,i5),padding='same',
                     input_shape=x_train1.shape[i1:]))
    model.add(LeakyReLU(alpha=al))   
    model.add(MaxPooling2D(pool_size=(i2,i2)))
    model.add(Dropout(dr))
    model.add(Conv2D(i196,(i5,i5)))
    model.add(LeakyReLU(alpha=al))  
    model.add(MaxPooling2D(pool_size=(i2,i2)))
    model.add(Dropout(dr))
    model.add(GlobalMaxPooling2D())  
    model.add(Dense(i1024))
    model.add(LeakyReLU(alpha=al))
    model.add(Dropout(2*dr))     
    model.add(Dense(i33,activation='softmax'))
    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',metrics=['accuracy'])   
    return model
model=model()
</script></div><br/>
<div class="linked"><script type="text/x-sage">
checkpointer=ModelCheckpoint(filepath=fw,verbose=i0,
                             save_best_only=True)
lr_reduction=ReduceLROnPlateau(monitor='val_loss',patience=i5,
                               verbose=i0,factor=fr)
history=model.fit(x_train1[:n],y_train1[:n],epochs=i64,
                  batch_size=i64,verbose=i2,
                  validation_data=(x_valid1,y_valid1),
                  callbacks=[checkpointer,lr_reduction])
</script></div><br/>  
<div class="linked"><script type="text/x-sage">
history_plot(history)
model.load_weights(fw)
model.evaluate(x_test1,y_test1)
</script></div><br/>
<p>Grayscale Images</p><br/>
<div class="linked"><script type="text/x-sage">

</script></div><br/> 
<div class="linked"><script type="text/x-sage">

</script></div><br/>
<div class="linked"><script type="text/x-sage">

</script></div><br/>
  </body>
</html>      